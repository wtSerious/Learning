{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "emmmmmmmmmmm...............\n",
    "原本是想写logistic回归，写着写着。。就写歪了。。。。写成了线性回归。。。\n",
    "这相当于是第一篇博客，所以想着把很多东西都纳入进去，也方便自己回顾一些基础内容。、\n",
    "写博客还是很耗时间的。。感觉自己想写的东西还远远没完，时间来不及，作业交了再说！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 什么是回归\n",
    "目前我把它理解为：已知有输入 {$x_{1},x_{2},...,x_{n}$} 和线性函数 $f(x)$。根据函数 $f(x)$ 和输入 $X$ 可以得到输出 {$y_{1},y_{2},...,y_{n}$}。以上过程是已知输入和函数来得到相应的输出，现在我们的目标是通过输入 $X$ 和输出 $Y$ 来寻找和拟合 $f(x)$ 函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义 $f(x)$\n",
    "通常一个线性函数 $f(x)$ 可以写成 $f(x)$ = $a + bx + cx^{2} + dx^{3}+..$ 把它写成矩阵的形式就是 $f(x)$ = $WX + b$\n",
    "从这个式子可知，一个线性函数由 $W$ 和 $b$ 这两个参数决定，不同的 $W$ 和不同的 $b$ 都会产生不同的线性函数。显然我们只要找到正确 $W$ 和 $b$ 就是找到了正确的 $f(x)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 判断是否找到了正确的 $f(x)$\n",
    "首先，我们不应该急于思考怎么找 $f(x)$ 而是先解决如何判断一个我们千方百计的找到的$f^{'}(x)$,就是正确的 $f(x)$ 的问题。\n",
    "一个很显然的想法就是通过判断 输入 X 通过 $f^{‘}(x)$ 是否产生了与真实输出 $Y$ 一样的输出。于是把 $Y^{'}$ 与真实输出 $Y$ 的差异作为衡量 $f^{'}(x)$ 拟合 $f(x)$ 程度的标准。然后就是怎么量化两个数据间的差异，然后就出现了一个很常用的函数————均方误差，$\\tfrac{1}{n}$$\\sum_{n}(y_{i}-f(x_{i})^{'})^{2}$,这就是损失函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如何找正确的 $f(x)$\n",
    "已知损失函数 $loss(w,b)$ $ = \\tfrac{1}{n}$$\\sum_{n}(y_{i}-\n",
    "f(x_{i})^{'})^{2}$,$w$ 和 $b$ 是损失函数的参数，也是我们需要优化的参数。可以发现损失函数越小说明 $f^{'}(x)$ 越接近正确的 $f(x)$，那么我们的目标就是使损失函数最小，于是就出现了梯度下降方法。\n",
    "\n",
    "在学习高数时，对一维函数 $g(x)$ 进行求导时，就是在 $y = g(x)$ 处画了条切线，切线的斜率就是导数，导数的正负表示这个 $g(x)$ 在点 $x$ 处是在增大还是减小。如果导数是正，我们只要减小 $x$ 就能减小 $y = g(x)$，反之，增大 $x$。总而言之，$x$的移动反向总是与梯度方向相反, $x^{new} = x^{old} - \\tfrac{\\Delta g(x))}{\\Delta x^{old}}$,这就是梯度下降法，每次都通过移动 $x$ 达到函数下降，经过多次的迭代后可以达到一个好的效果。\n",
    "\n",
    "现在回到对损失函数的讨论,在参数空间中任意的取一组参数 $(w^{'},b^{'})$,当然我们没有这个运气直接取到正确的 $(w,b)$，通常都会得到较坏的结果。然后把损失函数 $loss(w^{'},b^{'})$ 对 $w$ 和 $b$求偏导于是得到损失函数在(w^{'},b^{b})下对 $w$ 和 $b$ 两个方向的增减情况，现在我们的参数的二维的，于是我们能得到一个切平面,如果 $W$ 的维度不是一而是更高的，那么就是一个超平面。运用梯度下降思想，使 $w$ 每次都向使损失函数下降移动的方向 $w^{new} = w^{'} - \\frac{\\Delta loss(w,b))}{\\Delta w^{'}}$,同理得到$b^{new} = b^{'} - \\frac{\\Delta loss(w,b))}{\\Delta b^{'}}$。"
   ]
  },
  {
   "attachments": {
    "%E6%97%A0%E6%A0%87%E9%A2%98.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 往下降方向移动多少\n",
    "现在我们已经有了一个似乎很好的方法来下降损失函数，下面考虑一个情况:\n",
    "![%E6%97%A0%E6%A0%87%E9%A2%98.png](attachment:%E6%97%A0%E6%A0%87%E9%A2%98.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x^{'}$ 是初始点，如果 $x$ 的移动距离只依赖于梯度的大小，$x^{'}$ 很可能就只移动到 $x_{1}$ ，$x_{1}$ 的梯度比 $x^{'}$ 的梯度小，移动的距离会更小，就算继续进行梯度下降，也无法到达最小值点 $x_{'}$，只能到达局部极小值点，梯度为0，损失不再减小。为了解决这个问题，不妨人为的设计一个参数来控制移动距离的大小，这就是学习率 $\\eta$，用它来放大或缩小移动距离。于是梯度更新变成了 $x^{new} = x^{old} - \\eta  \\frac{\\Delta g^{'}(x))}{\\Delta x^{old}}$。\n",
    "引入学习率 $\\eta$ 后，又会产生一个问题， $\\eta$ 应该多大，如果 $\\eta$ 过大会导致 ${x^{'}}$ 移动到 ${x_{3}}$，然后 ${x_{3}}$ 又会移动到${x^{'}}$那边去，就这样两边旋转跳跃~所以设计如何设计一个好的学习率也是一个很重要的问题。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
