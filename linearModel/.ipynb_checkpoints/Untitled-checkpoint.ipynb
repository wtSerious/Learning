{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "import load_MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用写好的load_MNIST模块中的load_data方法导入MNIST数据集\n",
    "- Parameter\n",
    "    - file_name:list \n",
    "        四个文件路径\n",
    "        - list[0]: 训练集的图片数据文件路径\n",
    "        - list[1]: 测试集的图片数据文件路径\n",
    "        - list[2]: 训练集的标签文件路径\n",
    "        - list[3]: 测试集的标签文件路径\n",
    "- Return\n",
    "    - train_data: numpy array\n",
    "    - train_labels: list\n",
    "    - test_data: numpy array\n",
    "    - test_labels: list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = [\"C:/Users/wtser/Desktop/learnData/data/Mnist/train-images.idx3-ubyte\",\n",
    "       \"C:/Users/wtser/Desktop/learnData/data/Mnist/t10k-images.idx3-ubyte\",\n",
    "       \"C:/Users/wtser/Desktop/learnData/data/Mnist/train-labels.idx1-ubyte\",\n",
    "       \"C:/Users/wtser/Desktop/learnData/data/Mnist/t10k-labels.idx1-ubyte\"]  \n",
    "\n",
    "train_data,train_labels,test_data,test_labels = load_MNIST.load_data(file)\n",
    "\n",
    "#把训练集的标签进行改变，比如x被分为第二类，y = 1，但为了后续\n",
    "train_y = LabelBinarizer().fit_transform(train_labels)\n",
    "\n",
    "#为测试集和训练集的数据增加一维当作偏置值\n",
    "train_data = np.c_[train_data,np.ones([train_data.shape[0],1])]\n",
    "test_data = np.c_[test_data,np.ones([test_data.shape[0],1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 785) (60000, 1)\n",
      "[[0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape,np.ones([train_data.shape[0],1]).shape)\n",
    "print(np.c_[train_data,np.ones([train_data.shape[0],1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logisticRegression_softMax:\n",
    "    def __init__(self,size):\n",
    "        \"\"\"\n",
    "        初始化模型\n",
    "        \n",
    "        Parameters:\n",
    "        ___________\n",
    "        size: list of shape\n",
    "            权重的规模\n",
    "        \"\"\"\n",
    "        \n",
    "        self.W = np.random.rand(size[0],size[1])\n",
    "        \n",
    "    def softMax(self,x):\n",
    "        max_dict = [ [np.max(i)] for i in x]\n",
    "        print(max_dict)\n",
    "        print((x-max_dict))\n",
    "        x = np.exp((x-max_dict))\n",
    "        print(x)\n",
    "        return np.array([ i/np.sum(i) for i in x])\n",
    "    \n",
    "    def softMax_deriv(self,x,y):\n",
    "        return x - y\n",
    "    \n",
    "    def fit(self,X,Y,batch_size,epoch,alpha,count=10):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "        \n",
    "        Parameters:\n",
    "        ___________\n",
    "        X:numpy array of shape\n",
    "        Y:list of label\n",
    "        batch_size: 批量梯度下降法的批量大小\n",
    "        epoch:训练次数\n",
    "        alpha:学习率\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(epoch):\n",
    "            batch_index = np.random.choice(X.shape[0],batch_size,True)\n",
    "            batch_x = X[batch_index,:]\n",
    "            batch_y = Y[batch_index]\n",
    "            \n",
    "            self.train(batch_x,batch_y,alpha)\n",
    "            if i == 0 or (i+1)%count ==0:\n",
    "                print(\"i-loss\",self.calculate_loss(X,Y))\n",
    "    \n",
    "    def train(self,x,y,alpha):\n",
    "        net = x.dot(self.W)\n",
    "        print(net)\n",
    "        out = self.softMax(net)\n",
    "        print(out)\n",
    "        deriv = self.softMax_deriv(out,y)\n",
    "\n",
    "        self.W -= 1.0/x.shape[0] * alpha * x.T.dot(deriv)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        net = x.dot(self.W)\n",
    "        out = self.softMax(net)\n",
    "        return out\n",
    "    \n",
    "    def calculate_loss(self,x,y):\n",
    "        predictions = self.predict(x)\n",
    "        print(predictions[0])\n",
    "        loss = -sum(np.sum(y*np.log(predictions),1))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = logisticRegression_softMax([train_data.shape[1],10])\n",
    "# print(model.W)\n",
    "# model.fit(train_data[:1000],train_y[:1000],batch_size=10,epoch=1,alpha=0.01,count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15230.02376196]]\n",
      "[[-1368.45550772  -956.09698266     0.          -240.41733985\n",
      "     -8.65112046 -1016.16308063   -59.26012763  -642.87146556\n",
      "   -962.71907742  -417.81344914]]\n",
      "[[0.00000000e+000 0.00000000e+000 1.00000000e+000 3.87325374e-105\n",
      "  1.74930735e-004 0.00000000e+000 1.83507396e-026 6.37484947e-280\n",
      "  0.00000000e+000 3.51499388e-182]]\n",
      "[[0.00000000e+000 0.00000000e+000 9.99825100e-001 3.87257631e-105\n",
      "  1.74900140e-004 0.00000000e+000 1.83475301e-026 6.37373451e-280\n",
      "  0.00000000e+000 3.51437911e-182]]\n"
     ]
    }
   ],
   "source": [
    "print(model.softMax(np.array([[13861.56825424,14273.9267793,15230.02376196,14989.60642211\n",
    "  ,15221.3726415,14213.86068133,15170.76363433,14587.1522964,\n",
    "  14267.30468454,14812.21031282]])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
