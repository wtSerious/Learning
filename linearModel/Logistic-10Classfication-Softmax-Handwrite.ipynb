{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "import load_MNIST\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用写好的load_MNIST模块中的load_data方法导入MNIST数据集\n",
    "- Parameter\n",
    "    - file_name:list \n",
    "        四个文件路径\n",
    "        - list[0]: 训练集的图片数据文件路径\n",
    "        - list[1]: 测试集的图片数据文件路径\n",
    "        - list[2]: 训练集的标签文件路径\n",
    "        - list[3]: 测试集的标签文件路径\n",
    "- Return\n",
    "    - train_data: numpy array\n",
    "    - train_labels: list\n",
    "    - test_data: numpy array\n",
    "    - test_labels: list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "file = [\"C:/Users/wtser/Desktop/learnData/data/Mnist/train-images.idx3-ubyte\",\n",
    "       \"C:/Users/wtser/Desktop/learnData/data/Mnist/t10k-images.idx3-ubyte\",\n",
    "       \"C:/Users/wtser/Desktop/learnData/data/Mnist/train-labels.idx1-ubyte\",\n",
    "       \"C:/Users/wtser/Desktop/learnData/data/Mnist/t10k-labels.idx1-ubyte\"]  \n",
    "\n",
    "train_data,train_labels,test_data,test_labels = load_MNIST.load_data(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把图片数据进行标准化，如果不进行标准化，在未进行softmax之前，每个分类的输出相差过大，导致利用softmax时，结果时某一个分l类的输出为1，其他都是0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对数据进行预处理\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data)\n",
    "train_data = scaler.transform(train_data)\n",
    "test_data = scaler.transform(test_data)\n",
    "\n",
    "#把训练集的标签进行改变，比如x被分为第二类，y = 1，变成[0,1,0,0,0,0,0,0,0,0]\n",
    "train_y = LabelBinarizer().fit_transform(train_labels)\n",
    "\n",
    "#为测试集和训练集的数据增加一维当作偏置值\n",
    "train_data = np.c_[train_data,np.ones([train_data.shape[0],1])]\n",
    "test_data = np.c_[test_data,np.ones([test_data.shape[0],1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logisticRegression_softMax:\n",
    "    def __init__(self,size):\n",
    "        \"\"\"\n",
    "        初始化模型\n",
    "        \n",
    "        Parameters:\n",
    "        ___________\n",
    "        size: list of shape\n",
    "            权重的规模\n",
    "        \"\"\"\n",
    "        \n",
    "        self.W = np.random.rand(size[0],size[1])\n",
    "        \n",
    "    def softMax(self,x):\n",
    "        \"\"\"\n",
    "        在用softmax函数时，为了避免出现分子为non或粪分母为0出现数值越界，把每个output(i) - max(output(i\n",
    "        \"\"\"\n",
    "        max_dict = [ [np.max(i)] for i in x]\n",
    "        x = np.exp((x-max_dict))\n",
    "        return np.array([ i/np.sum(i) for i in x])\n",
    "    \n",
    "    def softMax_deriv(self,x,y):\n",
    "        return x - y\n",
    "    \n",
    "    def fit(self,X,Y,batch_size,epoch,alpha,count=10,reg=1):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "        \n",
    "        Parameters:\n",
    "        ___________\n",
    "        X:numpy array of shape\n",
    "        Y:list of label\n",
    "        batch_size: 批量梯度下降法的批量大小\n",
    "        epoch:训练次数\n",
    "        alpha:学习率\n",
    "        reg:正则化惩罚系数\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(epoch):\n",
    "            batch_index = np.random.choice(X.shape[0],batch_size,True)\n",
    "            batch_x = X[batch_index,:]\n",
    "            batch_y = Y[batch_index]\n",
    "            \n",
    "            self.train(batch_x,batch_y,alpha,reg)\n",
    "            if i == 0 or (i+1)%count ==0:\n",
    "                \n",
    "                pre = self.predict(test_data[:250])\n",
    "                pre = np.argmax(pre,axis=1)\n",
    "                print(\"i-loss\",self.calculate_loss(X,Y),\"acc\",self.calluclate_acc(pre,test_labels[:250]))\n",
    "    \n",
    "    def train(self,x,y,alpha,reg):\n",
    "        net = x.dot(self.W)\n",
    "        out = self.softMax(net)\n",
    "        deriv = self.softMax_deriv(out,y)\n",
    "\n",
    "        self.W -= alpha * reg +1.0/x.shape[0] * alpha * x.T.dot(deriv)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        net = x.dot(self.W)\n",
    "        out = self.softMax(net)\n",
    "        return out\n",
    "    \n",
    "    def calculate_loss(self,x,y):\n",
    "        predictions = self.predict(x)\n",
    "        loss = -sum(np.sum(y*np.log(predictions),1))\n",
    "        return loss\n",
    "    \n",
    "    def calluclate_acc(self,pre,y):\n",
    "        result = np.array(pre)==np.array(y)\n",
    "        return (1.0*sum(result)/len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i-loss 10767.341903764725 acc 0.064\n",
      "i-loss 464.20806925064744 acc 0.736\n",
      "i-loss 219.3904015471753 acc 0.744\n",
      "i-loss 130.36061205838934 acc 0.78\n",
      "i-loss 85.15694682924386 acc 0.776\n",
      "i-loss 58.60515017850914 acc 0.776\n",
      "i-loss 42.36124471578768 acc 0.788\n",
      "i-loss 32.62104918034619 acc 0.804\n",
      "i-loss 26.620028630328576 acc 0.816\n",
      "i-loss 22.428863774889752 acc 0.816\n",
      "i-loss 19.4790466498094 acc 0.82\n",
      "i-loss 17.179435538897334 acc 0.82\n",
      "i-loss 15.436972585837893 acc 0.82\n",
      "i-loss 14.003853401520432 acc 0.82\n",
      "i-loss 12.82894735501835 acc 0.82\n",
      "i-loss 11.858251667531894 acc 0.82\n",
      "i-loss 11.018649058489745 acc 0.82\n",
      "i-loss 10.296883482737268 acc 0.82\n",
      "i-loss 9.6714117483202 acc 0.82\n",
      "i-loss 9.120178080944784 acc 0.824\n",
      "i-loss 8.627534235124948 acc 0.824\n",
      "i-loss 8.184276254801784 acc 0.824\n",
      "i-loss 7.792921077363783 acc 0.824\n",
      "i-loss 7.433603956513402 acc 0.824\n",
      "i-loss 7.110057552820278 acc 0.824\n",
      "i-loss 6.8151209341121985 acc 0.824\n",
      "i-loss 6.541976914729608 acc 0.824\n",
      "i-loss 6.29160045074276 acc 0.824\n",
      "i-loss 6.061678893915398 acc 0.824\n",
      "i-loss 5.844524605478768 acc 0.824\n",
      "i-loss 5.646241236260589 acc 0.824\n"
     ]
    }
   ],
   "source": [
    "model = logisticRegression_softMax([train_data.shape[1],10])\n",
    "model.fit(train_data[:1000],train_y[:1000],batch_size=100,epoch=30000,alpha=0.01,count=1000,reg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.90      0.92        20\n",
      "          1       0.94      0.91      0.93        35\n",
      "          2       0.75      0.69      0.72        26\n",
      "          3       0.78      0.75      0.77        24\n",
      "          4       0.82      0.84      0.83        32\n",
      "          5       0.84      0.81      0.82        26\n",
      "          6       0.68      0.83      0.75        18\n",
      "          7       0.90      0.84      0.87        31\n",
      "          8       0.86      0.86      0.86        14\n",
      "          9       0.70      0.79      0.75        24\n",
      "\n",
      "avg / total       0.83      0.82      0.83       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pre = model.predict(test_data[:250])\n",
    "pre = np.argmax(pre,axis=1)\n",
    "print(classification_report(pre,test_labels[:250]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
