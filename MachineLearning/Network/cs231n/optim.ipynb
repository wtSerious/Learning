{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_momentum(w,dw,config=None):\n",
    "    \"\"\"\n",
    "    Performs stochastic gradient descent with momentum.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    config format:\n",
    "        learning_rate:\n",
    "        momentum:\n",
    "        velocity:\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    \n",
    "    config.setdefault('learning_rate',1e-2)\n",
    "    config.setdefault('momentum',0.9)\n",
    "    v = config.get('velocity',np.zeros_like(w))\n",
    "    \n",
    "    next_w = None\n",
    "    v = config['momentum'] * v - config['learning_rate']*dw\n",
    "    next_w = w + v\n",
    "    config['velocity'] = v\n",
    "    \n",
    "    return next_w,config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(w, dw, config=None):\n",
    "  \"\"\"\n",
    "  Performs vanilla stochastic gradient descent.\n",
    "\n",
    "  config format:\n",
    "  - learning_rate: Scalar learning rate.\n",
    "  \"\"\"\n",
    "  if config is None: config = {}\n",
    "  config.setdefault('learning_rate', 1e-2)\n",
    "\n",
    "  w -= config['learning_rate'] * dw\n",
    "  return w, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(x, dx, config=None):\n",
    "  \"\"\"\n",
    "  Uses the Adam update rule, which incorporates moving averages of both the\n",
    "  gradient and its square and a bias correction term.\n",
    "\n",
    "  config format:\n",
    "  - learning_rate: Scalar learning rate.\n",
    "  - beta1: Decay rate for moving average of first moment of gradient.\n",
    "  - beta2: Decay rate for moving average of second moment of gradient.\n",
    "  - epsilon: Small scalar used for smoothing to avoid dividing by zero.\n",
    "  - m: Moving average of gradient.\n",
    "  - v: Moving average of squared gradient.\n",
    "  - t: Iteration number.\n",
    "  \"\"\"\n",
    "  if config is None: config = {}\n",
    "  config.setdefault('learning_rate', 1e-3)\n",
    "  config.setdefault('beta1', 0.9)\n",
    "  config.setdefault('beta2', 0.999)\n",
    "  config.setdefault('epsilon', 1e-8)\n",
    "  config.setdefault('m', np.zeros_like(x))\n",
    "  config.setdefault('v', np.zeros_like(x))\n",
    "  config.setdefault('t', 0)\n",
    "  \n",
    "  next_x = None\n",
    "  #############################################################################\n",
    "  # TODO: Implement the Adam update formula, storing the next value of x in   #\n",
    "  # the next_x variable. Don't forget to update the m, v, and t variables     #\n",
    "  # stored in config.                                                         #\n",
    "  #############################################################################\n",
    "  config['t'] += 1\n",
    "  config['m'] = config['beta1'] * config['m'] + (1 - config['beta1']) * dx\n",
    "  config['v'] = config['beta2'] * config['v'] + (1 - config['beta2']) * (dx**2)\n",
    "  mb = config['m'] / (1 - config['beta1']**config['t'])\n",
    "  vb = config['v'] / (1 - config['beta2']**config['t'])\n",
    "  next_x = x - config['learning_rate'] * mb / (np.sqrt(vb) + config['epsilon'])\n",
    "  #pass\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  \n",
    "  return next_x, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook optim.ipynb to python\n",
      "[NbConvertApp] Writing 2947 bytes to optim.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to python optim.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
