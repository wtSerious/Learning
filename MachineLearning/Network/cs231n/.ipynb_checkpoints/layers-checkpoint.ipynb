{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys;\n",
    "sys.path.append(\"E:/JupyterEnviroment/Learning/MachineLearning/Network/cs231n\")\n",
    "import numpy as np\n",
    "from fast_layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_forward(x,w,b):\n",
    "    \n",
    "    \"\"\"\n",
    "Compute the forward pass for affine layer\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "x: A numpy array containing input data,of shape(N,d_1,d_2,..,d_k) \n",
    "w: A numpy array of weights, of shape (D,M)\n",
    "b: A numpy array of biases, of shape (M,)\n",
    "\n",
    "Return a tuple of\n",
    "----------\n",
    "out: output, of shape(N,M)\n",
    "cache: (x,w,b)\n",
    "    \"\"\"\n",
    "    N = x.shape[0]\n",
    "    x_rsp = x.reshape(N,-1)\n",
    "    out = x_rsp.dot(w) + b\n",
    "    cache = (x,w,b)\n",
    "    return out,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_forward(x):\n",
    "    \"\"\"\n",
    "    Compute the forward pass for a layer of relu \n",
    "    \"\"\"\n",
    "    out = x*(x >= 0)\n",
    "    cache = x\n",
    "    return out,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm_forward(x, gamma, beta, bn_param):\n",
    "    mode = bn_param['mode']\n",
    "    eps = bn_param.get('eps', 1e-5)\n",
    "    momentum = bn_param.get('momentum', 0.9)\n",
    "\n",
    "    N, D = x.shape\n",
    "    running_mean = bn_param.get('running_mean', np.zeros(D, dtype=x.dtype))\n",
    "    running_var = bn_param.get('running_var', np.zeros(D, dtype=x.dtype))\n",
    "    \n",
    "    out, cache = None, None\n",
    "    if mode == 'train':\n",
    "        sample_mean = np.mean(x,axis = 0)\n",
    "        sample_var = np.var(x,axis = 0)\n",
    "        x_hat = (x-sample_mean)/(np.sqrt(sample_var+eps))\n",
    "        out = gamma * x_hat + beta\n",
    "        cache = (gamma,x,sample_mean,sample_var,eps,x_hat)\n",
    "        running_mean = momentum * running_mean + (1-momentum)*sample_mean\n",
    "        running_var = momentum * running_var + (1-momentum)*sample_var\n",
    "    elif mode == 'test':\n",
    "        scale = gamma / (np.sqrt(running_var  + eps))\n",
    "        out = x * scale + (beta - running_mean * scale)\n",
    "    else:\n",
    "        raise ValueError('Invalid forward batchnorm mode \"%s\"' % mode)\n",
    "        \n",
    "    bn_param['running_mean'] = running_mean\n",
    "    bn_param['running_var'] = running_var\n",
    "\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_forward(x,dropout_param):\n",
    "    p,mode = dropout_param['p'],dropout_param['mode']\n",
    "    if 'seed' in dropout_param:\n",
    "        np.random.seed(dropout_param['seed'])\n",
    "    mask = None\n",
    "    out = None\n",
    "    \n",
    "    if mode == 'train':\n",
    "        #在这里采用了在训练的时候就扩大了输出，因此在测试时不用\n",
    "        #缩小输出\n",
    "        mask = (np.random.rand(*x.shape)>=p)/(1-p)\n",
    "        out = x * mask\n",
    "    \n",
    "    elif mode == 'test':\n",
    "        out = x\n",
    "    cache = (dropout_param,mask)\n",
    "    out = out.astype(x.dtype,copy = False)\n",
    "    \n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_relu_forward(x,w,b):\n",
    "    \"\"\"\n",
    "    Convenience layer that perorms an affine transform \n",
    "    followed by Relu\n",
    "    \n",
    "    Parameter:\n",
    "    ----------\n",
    "    x: A numpy array of input data, of shape (N,M)\n",
    "    w: A numpy array of weight\n",
    "    \n",
    "    Return a typle of:\n",
    "    ----------\n",
    "    out: Output from the Relu\n",
    "    cache: Object to give to the backward pass\n",
    "    \"\"\"\n",
    "    z,af_cache = affine_forward(x,w,b)\n",
    "    out,relu_cahce = relu_forward(z)\n",
    "    cache = (af_cache,relu_cahce)\n",
    "    return out,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_bn_relu_forward(x,w,b,gamma,beta,bn_param):\n",
    "    a,fc_cache = affine_forward(x,w,b)\n",
    "    bn,bn_cache = batchnorm_forward(a,gamma,beta,bn_param)\n",
    "    out,relu_cache = relu_forward(bn)\n",
    "    cache = (fc_cache,bn_cache,relu_cache)\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward_naive(x,w,b,conv_params):\n",
    "    \"\"\"\n",
    "    A naive implementation of the forward pass for a convolutional layer.\n",
    "    \n",
    "    The input consists of N data point ,each with C channels, heights H and width\n",
    "    W. We convolve each input with F different filters,where each filter \n",
    "    spans all C channels and has height HH and width HH.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    - x: Input data of shape (N,C,H,W)\n",
    "    - w: Filter weights of shape (F,C,HH,WW)\n",
    "    - b: Biases,of shape(F,)\n",
    "    - conv_params: A dictionary with the follow keys:\n",
    "     - 'stride': The number of pixels between adjacent receptive fields in the\n",
    "     horizontal and vertical directions.\n",
    "     - 'pad': The number of pixels that will be used to zero-pad the input.\n",
    "     \n",
    "     Return a tuple of:\n",
    "     ------------------\n",
    "     - out: Output data ,of shape(N,F,H',W') where H' and W' were given by\n",
    "       H' = 1 + (H+2*pad-HH)/stride\n",
    "       W' = 1 + (W+2*pad-HH)/stride\n",
    "     - cache:(x,w,b,conv_param)\n",
    "   \"\"\"\n",
    "    N,C,H,W = x.shape[0],x.shape[1],x.shape[2],x.shape[3]\n",
    "    F,HH,WW = w.shape[0],w.shape[2],w.shape[3]\n",
    "    stride,pad = conv_params['stride'],conv_params['pad']\n",
    "    data = np.pad(x,((0,),(0,),(pad,),(pad,)),mode='constant',constant_values = 0)\n",
    "    _H = int((H + 2 * pad - HH) / stride) + 1\n",
    "    _W = int((W + 2 * pad - WW) / stride) + 1\n",
    "    out = np.zeros((N,F,_H,_W))\n",
    "    \n",
    "    for i in range(_H):\n",
    "        for j in range(_W):\n",
    "            x_mask = data[:,:,i*stride:i*stride+HH,j*stride:j*stride+WW]\n",
    "            for k in range(F):\n",
    "                out[:,k,i,j] = np.sum(x_mask*w[k,:,:,:],axis=(1,2,3))\n",
    "                \n",
    "    out = out + (b)[None,:,None,None]\n",
    "    cache = (x,w,b,conv_params)\n",
    "    return out,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool_forward_naive(x, pool_param):\n",
    "    \"\"\"\n",
    "    A naive implementation of the forward pass for a max pooling layer.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, C, H, W)\n",
    "    - pool_param: dictionary with the following keys:\n",
    "    - 'pool_height': The height of each pooling region\n",
    "    - 'pool_width': The width of each pooling region\n",
    "    - 'stride': The distance between adjacent pooling regions\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data\n",
    "    - cache: (x, pool_param)\n",
    "    \"\"\"\n",
    "    N,C,H,W = x.shape[0],x.shape[1],x.shape[2],x.shape[3]\n",
    "    HH,WW,stride = pool_param['pool_height'],pool_param['pool_width'],pool_param['stride']\n",
    "    _H = int((H-HH)/stride)+1\n",
    "    _W = int((W-WW)/stride)+1\n",
    "    out = np.zeros((N,C,_H,_W))\n",
    "    \n",
    "    for i in range(_H):\n",
    "        for j in range(_W):\n",
    "            x_mask = x[:,:,i*stride:i*stride+HH,j*stride:j*stride+WW]\n",
    "            out[:,:,i,j] = np.max(x_mask, axis=(2,3)) \n",
    "    \n",
    "    cache = (x,pool_param)\n",
    "    return out,cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_relu_forward(x, w, b, conv_param):\n",
    "    \"\"\"\n",
    "    A convenience layer that performs a convolution followed by a ReLU.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input to the convolutional layer\n",
    "    - w, b, conv_param: Weights and parameters for the convolutional layer\n",
    "  \n",
    "    Returns a tuple of:\n",
    "    - out: Output from the ReLU\n",
    "    - cache: Object to give to the backward pass\n",
    "    \"\"\"\n",
    "    a, conv_cache = conv_forward_fast(x, w, b, conv_param)\n",
    "    out, relu_cache = relu_forward(a)\n",
    "    cache = (conv_cache, relu_cache)\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_relu_pool_forward(x, w, b, conv_param, pool_param):\n",
    "    \"\"\"\n",
    "    Convenience layer that performs a convolution, a ReLU, and a pool.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input to the convolutional layer\n",
    "    - w, b, conv_param: Weights and parameters for the convolutional layer\n",
    "    - pool_param: Parameters for the pooling layer\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output from the pooling layer\n",
    "    - cache: Object to give to the backward pass\n",
    "    \"\"\"\n",
    "    con_out,con_cache = conv_forward_fast(x=x,w=w,b=b,conv_param=conv_param)\n",
    "    r_out,r_cache = relu_forward(con_out)\n",
    "    out ,pool_cache = max_pool_forward_fast(r_out,pool_param)\n",
    "    cache = (con_cache,r_cache,pool_cache)\n",
    "    return out,cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool_backward_naive(dout, cache):\n",
    "    \"\"\"\n",
    "    A naive implementation of the backward pass for a max pooling layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives\n",
    "    - cache: A tuple of (x, pool_param) as in the forward pass.\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    x,pool_param = cache\n",
    "    N,C,H,W = x.shape\n",
    "    HH,WW,stride = pool_param['pool_height'],pool_param['pool_width'],pool_param['stride']\n",
    "    _H = int((H-HH)/stride)+1\n",
    "    _W = int((W-WW)/stride)+1\n",
    "    dx = np.zeros_like(x)\n",
    "    \n",
    "    for i in range(_H):\n",
    "        for j in range(_W):\n",
    "            x_mask = x[:,:,i*stride:i*stride+HH,j*stride:j*stride+WW]\n",
    "            x_mask_max = np.max(x_mask,axis=(2,3))\n",
    "            temp_binary_mask = (x_mask==(x_mask_max)[:,:,None,None])\n",
    "            dx[:,:,i*stride:i*stride+HH,j*stride:j*stride+WW] += (dout[:,:,i,j])[:,:,None,None]*temp_binary_mask\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_backward(dout,cache):\n",
    "    \"\"\"\n",
    "    Compute the backward  pass for an affine layers.py\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dout: Upstream derivative of shape(N,M)\n",
    "    cache: Tuple of\n",
    "        x: Input data\n",
    "        w: Weight, of shape(D,M)\n",
    "        \n",
    "    Return:\n",
    "    -----------\n",
    "    dx: Gradient with respect to x, of shape(N,d_1,d_2,...,d_k)\n",
    "    db: Gradient with respect to b, of shape(M,)\n",
    "    \"\"\"\n",
    "    x,w,b = cache\n",
    "    x_rsp = x.reshape(x.shape[0],-1)\n",
    "    dx = dout.dot(w.T)\n",
    "    dx = dx.reshape(*x.shape)\n",
    "    dw = x_rsp.T.dot(dout)\n",
    "    db = np.sum(dout,axis= 0)\n",
    "    \n",
    "    return dx,dw,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dout,cache):\n",
    "    dx = (cache>=0 ) * dout\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm_backward(dout,cache):\n",
    "    dx, dgamma, dbeta = None, None, None\n",
    "    gamma, x, u_b, sigma_squared_b, eps, x_hat = cache\n",
    "    N = x.shape[0]\n",
    "\n",
    "    dx_1 = gamma * dout\n",
    "    dx_2_b = np.sum((x - u_b) * dx_1, axis=0)\n",
    "    dx_2_a = ((sigma_squared_b + eps) ** -0.5) * dx_1\n",
    "    dx_3_b = (-0.5) * ((sigma_squared_b + eps) ** -1.5) * dx_2_b\n",
    "    dx_4_b = dx_3_b * 1\n",
    "    dx_5_b = np.ones_like(x) / N * dx_4_b\n",
    "    dx_6_b = 2 * (x - u_b) * dx_5_b\n",
    "    dx_7_a = dx_6_b * 1 + dx_2_a * 1\n",
    "    dx_7_b = dx_6_b * 1 + dx_2_a * 1\n",
    "    dx_8_b = -1 * np.sum(dx_7_b, axis=0)\n",
    "    dx_9_b = np.ones_like(x) / N * dx_8_b\n",
    "    dx_10 = dx_9_b + dx_7_a\n",
    "\n",
    "    dgamma = np.sum(x_hat * dout, axis=0)\n",
    "    dbeta = np.sum(dout, axis=0)\n",
    "    dx = dx_10\n",
    "    return dx, dgamma, dbeta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_backward(dout,cache):\n",
    "    dropout_param,mask = cache\n",
    "    mode = dropout_param['mode']\n",
    "    \n",
    "    dx = None\n",
    "    if mode == 'train':\n",
    "        dx = dout * mask\n",
    "    elif mode == 'test':\n",
    "        dx = dout\n",
    "    return dx    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward_naive(dout, cache):\n",
    "    \"\"\"\n",
    "    A naive implementation of the backward pass for a convolutional layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives.\n",
    "    - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x\n",
    "    - dw: Gradient with respect to w\n",
    "    - db: Gradient with respect to b\n",
    "    \"\"\"\n",
    "    x,w,b,conv_params = cache\n",
    "    N,C,H,W = x.shape[0],x.shape[1],x.shape[2],x.shape[3]\n",
    "    F,HH,WW = w.shape[0],w.shape[1],w.shape[2]\n",
    "    stride,pad = conv_params['stride'],conv_params['pad']\n",
    "    _H = int((H+2*pad-HH)/stride) + 1\n",
    "    _W = int((H+2*pad-HH)/stride) + 1\n",
    "    x_pad = np.pad(x, ((0,), (0,), (pad,), (pad,)), mode='constant', constant_values=0)\n",
    "    dx = np.zeros_like(x)\n",
    "    dx_pad = np.zeros_like(x_pad)\n",
    "    dw = np.zeros_like(w)\n",
    "    db = np.zeros_like(b)\n",
    "    db = np.sum(dout, axis = (0,2,3))\n",
    "    for i in range(_H):\n",
    "        for j in range(_W):\n",
    "            x_mask = x_pad[:,:,i*stride:(i*stride)+HH,j*stride:(j*stride)+WW]\n",
    "            for k in range(F):\n",
    "                #d[i,j] = dout[i,j] * x_mask[:,:]\n",
    "                dw[k,:,:,:] += np.sum(x_mask[:,:,:,:]*(dout[:, k, i, j])[:,None,None,None],axis=0)\n",
    "            for n in range(N):\n",
    "                \n",
    "                dx_pad[n,:,i*stride:(i*stride)+HH,j*stride:(j*stride)+WW] += np.sum(\n",
    "                    (dout[n,:,i,j])[:,None,None,None]*w[:,:,:,:],axis=0)\n",
    "    \n",
    "    dx = dx_pad[:,:,pad:-pad,pad:-pad]\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for the conv-relu convenience layer.\n",
    "    \"\"\"\n",
    "    conv_cache, relu_cache = cache\n",
    "    da = relu_backward(dout, relu_cache)\n",
    "    dx, dw, db = conv_backward_fast(da, conv_cache)\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_relu_backward(dout,cache):\n",
    "    af_cache,relu_cache = cache\n",
    "    dr = relu_backward(dout,relu_cache)\n",
    "    dx,dw,db = affine_backward(dr,af_cache)\n",
    "    return dx,dw,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_bn_relu_backward(dout, cache):\n",
    "    fc_cache,bn_cache,relu_cache = cache\n",
    "    dbn = relu_backward(dout,relu_cache)\n",
    "    da,dgammg,dbeta = batchnorm_backward(dbn,bn_cache)\n",
    "    dx,dw,db = affine_backward(da,fc_cache)\n",
    "    return dx,dw,db,dgammg,dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_relu_pool_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for the conv-relu-pool convenience layer\n",
    "    \"\"\"\n",
    "    con_cache,r_cache,pool_cache = cache\n",
    "    pool_back_out = max_pool_backward_fast(dout,pool_cache)\n",
    "    r_back_out = relu_backward(pool_back_out,r_cache)\n",
    "    dx,dw,db = conv_backward_fast(r_back_out,con_cache)\n",
    "    return dx,dw,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_loss(x,y):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    probs = np.exp(x-np.max(x,axis=1,keepdims=True))\n",
    "    probs /= np.sum(probs,axis=1,keepdims=True)\n",
    "    N = x.shape[0]\n",
    "    loss = -np.sum(np.log([probs[np.arange(N),y]]))/N\n",
    "    dx = probs.copy()\n",
    "    dx[np.arange(N),y] -= 1\n",
    "    dx /= N\n",
    "    return loss, dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook layers.ipynb to python\n",
      "[NbConvertApp] Writing 12744 bytes to layers.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to python layers.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
