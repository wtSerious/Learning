{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys;\n",
    "sys.path.append(\"E:/JupyterEnviroment/Learning/MachineLearning/Network/\")\n",
    "import numpy as np\n",
    "from cs231n.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class TwoLayerNet(object):\n",
    "    \"\"\"\n",
    "    A two-layer fully-connetcted neural network \n",
    "    with Relu nonlinearity and soft_loss that uses\n",
    "    a modular layer design. we assum an input dimension\n",
    "    of D , a hidden dimension of H, and perform classification\n",
    "    over C classes.\n",
    "    \n",
    "    The architecure should be affine-relu-affine-softmax.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,input_dim = 3*32*32,hidden_dim = 100,\n",
    "                num_classes = 10,weight_scale=1e-3,reg=0.0):\n",
    "        \"\"\"\n",
    "         Initialize the netwrok.\n",
    "     \n",
    "         Parameters:\n",
    "         -----------\n",
    "         input_dim: An interger giving the size of the input\n",
    "         hidden_dim: An interger giving the size of the hidden  layers\n",
    "         num_classes: An integer giving the number of classes to classify\n",
    "         weight_scale: Scalar giving the standard deviation for random\n",
    "         reg: Scalar gibing L2 regularization strenth.\n",
    "         \"\"\"   \n",
    "        self.params = {}\n",
    "        self.reg = reg\n",
    "        \n",
    "        self.params['W1'] = weight_scale * np.random.randn(input_dim, hidden_dim)\n",
    "        self.params['b1'] = np.zeros(hidden_dim)\n",
    "        self.params['W2'] = weight_scale * np.random.randn(hidden_dim, num_classes)\n",
    "        self.params['b2'] = np.zeros(num_classes)\n",
    "    \n",
    "    def loss(self,X,y=None):\n",
    "       \"\"\"\n",
    "       Compute loss and gardient for a minibacht of data.\n",
    "       \n",
    "       Parameters:\n",
    "       -----------\n",
    "       X: A numpy array of input data, of shape(,)\n",
    "       y: A Array of labels\n",
    "       \n",
    "       Returns:\n",
    "       -----------\n",
    "       if y is not None, then rum a training-time forward\n",
    "       and backward pass and return a tuple of (loss,grads)\n",
    "       and self.params\n",
    "       \"\"\" \n",
    "       ar1_out,ar1_cache = affine_relu_forward(X,self.params['W1'],self.params['b1'])\n",
    "       a2_out,a2_cache = affine_forward(ar1_out,self.params['W2'],self.params['b2'])\n",
    "       scores = a2_out\n",
    "        \n",
    "        #if y is None then we are in test mode so just return scores\n",
    "       if y is None:\n",
    "        \n",
    "        return scores\n",
    "        \n",
    "       loss,grads = 0,{}\n",
    "       loss,dscores = soft_loss(scores,y)\n",
    "       loss = loss + 0.5 * self.reg * np.sum(self.params['W1'] * self.params['W1']) + 0.5 * self.reg * np.sum(self.params['W2'] * self.params['W2'])\n",
    "       dx2,dw2,db2 = affine_backward(dscores,a2_cache)\n",
    "       grads['W2'] = dw2 + self.reg*self.params['W2']\n",
    "       grads['b2'] = db2\n",
    "       dx1,dw1,db1 = affine_relu_backward(dx2,ar1_cache)\n",
    "       grads['W1'] = dw1 + self.reg*self.params['W1']\n",
    "       grads['b1'] = db1 \n",
    "       return loss,grads\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class FullConnetctedNet(object):\n",
    "    \"\"\"\n",
    "     A full-connetcted neural network with an arbitrary numer\n",
    "    of hidden layers, Relu nonlinearities and a softmax loss funciton.\n",
    "     This will also implement dropout and batch normalization as options.\n",
    "    For L layers , the architecture will be \n",
    "     {affine - [batch norm] - relu - [dropout]} x (L-1) - affine - \n",
    "    softmax where batch normalization and dropout are optional,and\n",
    "    the {...} block is repeated L-1 times.\n",
    "    \n",
    "    Similar to the TwoLayerNet above, learnalble parameters are \n",
    "    stored in the self.params dictionary and will be learned using \n",
    "    the Solver class.\n",
    "    \"\"\"\n",
    "    def __init__(self,hidden_dims,input_dim = 3*32*32,num_classes = 10,\n",
    "                dropout = 0,use_batchnorm = False,reg = 0.0,weight_scale = 1e-2,\n",
    "                dtype = np.float32,seed = None):\n",
    "        \"\"\"\n",
    "        Initialize a new FullyConnetedNet.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        - hidden_dims: A list of integers giving the size of \n",
    "        each hidden layer.\n",
    "        - input_dim: An integer giving the size of the input.\n",
    "        - num_classes:An integer giving the number of classes to classify.\n",
    "        - dropout: Scaler between 0 and 1 giving dropout strength. If\n",
    "        dropout = 0 then the network should not use dropout at all.\n",
    "        - use_batchnorm: Whether or not use the network should use\n",
    "        normalization.\n",
    "        - reg: Scaler giving L2 regularization strength.\n",
    "        - weight_scale: Scaler giving the standard deviation for random\n",
    "        initialization of the weights.\n",
    "        - dtype: A numpy datatype object; all computations will be \n",
    "        preformed using this datatype. float32 is faster but less accurate.\n",
    "        use float64 for numeric gradient checking.\n",
    "        - seed: If not None, then pass this random seed to the dropout l\n",
    "        layers. This will make the dropout layers deteriminstic so we can \n",
    "        gradient the model.\n",
    "        \"\"\"\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.use_dropout = dropout > 0\n",
    "        self.reg = reg\n",
    "        self.num_layers = 1 + len(hidden_dims)\n",
    "        self.dtype = dtype\n",
    "        self.params = {}\n",
    "        \n",
    "        #initialize weights and b\n",
    "        layer_input_dim = input_dim\n",
    "        for i,h in enumerate(hidden_dims):\n",
    "            self.params['W%d'%(i+1)] = weight_scale * np.random.randn(layer_input_dim,h) \n",
    "            self.params['b%d'%(i+1)] = weight_scale * np.zeros(h)\n",
    "            if self.use_batchnorm:\n",
    "                self.params['gamma%d'%(i+1)] = np.ones(h)\n",
    "                self.params['beta%d'%(i+1)] = np.zeros(h)\n",
    "            layer_input_dim = h\n",
    "        self.params['W%d'%(self.num_layers)] = weight_scale * np.random.randn(layer_input_dim, num_classes)\n",
    "        self.params['b%d'%(self.num_layers)] = weight_scale * np.zeros(num_classes)\n",
    "        \n",
    "        #dropout\n",
    "        self.dropout_param = {}\n",
    "        if self.use_dropout:\n",
    "            self.dropout_param = {'mode': 'train','p': dropout}\n",
    "            if seed is not None:\n",
    "                self.dropout_param['seed'] = seed\n",
    "        \n",
    "        self.bn_params = []\n",
    "        if self.use_batchnorm:\n",
    "            for bn_param in self.bn_params:\n",
    "                bn_param['mode'] = mode\n",
    "        \n",
    "        for k,v in self.params.items():\n",
    "            self.params[k] = v.astype(dtype)\n",
    "    \n",
    "    def loss(self,X,y = None):\n",
    "        X = X.astype(self.dtype)\n",
    "        mode = 'test' if y is None else 'train'\n",
    "        \n",
    "        if self.dropout_param is not None:\n",
    "            self.dropout_param['mode'] = mode\n",
    "        if self.bn_params is not None:\n",
    "            self.bn_params['mode'] = mode\n",
    "            \n",
    "        scores = None\n",
    "        \n",
    "        layer_input = X\n",
    "        ar_cache = {}\n",
    "        dp_cache = {}\n",
    "        \n",
    "        for lay in range(self.num_layers-1):\n",
    "            if self.use_batchnorm:\n",
    "                layer_input,ar_cache[lay] = affine_bn_relu_forward(\n",
    "                layer_input,self.params['W%d'%(lay+1)],\n",
    "                self.params['b%d'%(lay+1)],self.params['gamma%d'%(lay+1)],\n",
    "                self.params['beta%d'%(lay+1)],self.bn_params[lay])\n",
    "            else:\n",
    "                layer_input, ar_cache[lay] = affine_relu_forward(x=layer_input,w=self.params['W%d'%lay+1]\n",
    "                                                                 ,b=self.params['b%d'%(lay+1)])\n",
    "            if self.use_dropout:\n",
    "                layer_input,dp_cache[lay] = dropout_forward(x=layer_input,dropout_param=self.dropout_param)\n",
    "        \n",
    "        #calculate the output of the last layer(output layer)      \n",
    "        arout,ar_cache[self.num_layers] = affine_forward(layer_input, self.params['W%d'%(self.num_layers)], self.params['b%d'%(self.num_layers)])\n",
    "        \n",
    "        scores = arout\n",
    "        \n",
    "        if mode =='test':\n",
    "            return scores\n",
    "        \n",
    "        loss,grad = 0.0,{}\n",
    "        \n",
    "        loss ,dscores = soft_loss(scores,y)\n",
    "        dhout = dscores\n",
    "        loss = loss + 0.5 * self.reg * np.sum(self.params['W%d'%(self.num_layers)] * self.params['W%d'%(self.num_layers)])\n",
    "        \n",
    "        #calculate the gradient of the output layer first\n",
    "        dx , dw , db = affine_backward(dhout , ar_cache[self.num_layers])\n",
    "        grads['W%d'%(self.num_layers)] = dw + self.reg * self.params['W%d'%(self.num_layers)]\n",
    "        grads['b%d'%(self.num_layers)] = db\n",
    "        dhout = dx\n",
    "        \n",
    "        #calculate the gradient of the hidden layers then\n",
    "        for idx in range(self.num_layers-1):\n",
    "            lay = self.num_layers - 1 - idx - 1\n",
    "            loss = loss = loss + 0.5 * self.reg * np.sum(self.params['W%d'%(lay+1)] * self.params['W%d'%(lay+1)])\n",
    "            if self.use_dropout:\n",
    "                dx = dropout_backward(dx,dp_cache[lay])\n",
    "            if self.use_batchnorm:\n",
    "                dx,dw,db,dgama,dbeta = affine_bn_relu_backward(dx,ar_cache[lay])\n",
    "            else:\n",
    "                dx,dw,db = affine_relu_backward(dx,ar_cache[lay])\n",
    "            \n",
    "            grads['W%d'%(lay+1)] = dw + self.reg*self.params['W%d'%(lay+1)]\n",
    "            grads['b%d'%(lay+1)] = db \n",
    "            if use_batchnorm:\n",
    "                grads['gamma%d'%(lay+1)] = dgama\n",
    "                grads['beta%d'%(lay+1)] = dbeta\n",
    "            dhout = dx\n",
    "            \n",
    "        return loss,grads\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook fc_net.ipynb to python\n",
      "[NbConvertApp] Writing 8994 bytes to fc_net.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to python fc_net.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
