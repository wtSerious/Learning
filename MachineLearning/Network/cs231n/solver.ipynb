{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys;\n",
    "sys.path.append(\"E:/JupyterEnviroment/Learning/MachineLearning/Network/cs231n/\")\n",
    "import optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver(object):\n",
    "    def __init__(self,model,data,**kwargs):\n",
    "        self.model = model\n",
    "        self.X_train = data[\"X_train\"]\n",
    "        self.y_train = data[\"y_train\"]\n",
    "        self.X_val,self.y_val = data[\"X_val\"],data[\"y_val\"]\n",
    "        self.update_rule = kwargs.pop(\"update_rule\",\"sgd_momentum\")\n",
    "        self.optim_config = kwargs.pop('optim_config',{})\n",
    "        self.lr_decay = kwargs.pop('lr_decay',1.0)\n",
    "        self.batch_size = kwargs.pop('batch_size',100)\n",
    "        self.num_epochs = kwargs.pop('num_epochs',10)\n",
    "        self.print_every = kwargs.pop('print_every',10)\n",
    "        self.verbose = kwargs.pop('verbose',True)\n",
    "       \n",
    "        # Throw an error if there are extra keyword arguments\n",
    "        if len(kwargs) > 0:\n",
    "            extra = ','.join('\"%s\"'%k for k in kwargs.keys())\n",
    "            raise ValueError('Unrecognized arguments %s'%extra)\n",
    "         \n",
    "        # Make sure the update rule exist,then replace the string\n",
    "        # name with  the actual function\n",
    "        if not hasattr(optim,self.update_rule):\n",
    "            raise ValueError('Invalid update_rule\"%s\"'%self.update_rule)\n",
    "        self.update_rule = getattr(optim,self.update_rule)\n",
    "        self._reset()\n",
    "        \n",
    "    def _reset(self):\n",
    "        \"\"\"\n",
    "        Set up some book-keeping variables for optimization.\n",
    "        Don't call this manually.\n",
    "        \"\"\"\n",
    "        #Set up some variable for book-keeping\n",
    "        self.epoch = 0\n",
    "        self.best_val_acc = 0\n",
    "        self.best_params = {}\n",
    "        self.loss_history = []\n",
    "        self.train_acc_history = []\n",
    "        self.val_acc_history = []\n",
    "        \n",
    "        #Make a deep copy of the optim_config for each parameters\n",
    "        self.optim_configs = {}\n",
    "        for p in self.model.params:\n",
    "            d = {k:v for k,v in self.optim_config.items()}\n",
    "            self.optim_configs[p] = d\n",
    "    \n",
    "    def _step(self):\n",
    "        \"\"\"\n",
    "        Make a single gradient update. This is called\n",
    "        by train() and should not be called manually.\n",
    "        \"\"\" \n",
    "        # Make a minibatch of trainning data\n",
    "        num_train = self.X_train.shape[0]\n",
    "        batch_mask = np.random.choice(num_train,self.batch_size)\n",
    "        X_batch = self.X_train[batch_mask]\n",
    "        y_batch = self.y_train[batch_mask]\n",
    "        \n",
    "        #Compute the loss and gradient\n",
    "        loss,grads = self.model.loss(X_batch,y_batch)\n",
    "        self.loss_history.append(loss)\n",
    "        for p,w in self.model.params.items():\n",
    "            dw = grads[p]\n",
    "            config = self.optim_configs[p]#这是啥\n",
    "            next_w,next_config = self.update_rule(w,dw,config)\n",
    "            self.model.params[p] = next_w\n",
    "            self.optim_configs[p] = next_config\n",
    "    \n",
    "    def check_accuracy(self,X,y,num_samples = None,batch_size = 100):\n",
    "        N = X.shape[0]\n",
    "        if num_samples is not None and N > batch_size:\n",
    "            mask = np.random.choice(N,num_samples)\n",
    "            N = num_samples\n",
    "            X = X[mask]\n",
    "            y = y[mask]\n",
    "            \n",
    "        #Compute predictions in batches\n",
    "        num_batches = int(N/batch_size)\n",
    "        if N % batch_size !=0:\n",
    "            num_batches += 1\n",
    "\n",
    "        y_pred = []\n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = (i+1) * batch_size\n",
    "            scores = self.model.loss(X[start:end])\n",
    "            y_pred.append(np.argmax(scores,axis = 1))\n",
    "        y_pred = np.hstack(y_pred)\n",
    "        acc = np.mean(y_pred==y)\n",
    "        \n",
    "        return acc\n",
    "    \n",
    "    def train(self):\n",
    "        num_train = self.X_train.shape[0]\n",
    "        \n",
    "        #计算整个数据集有多少个 batch_size 大小\n",
    "        iterations_per_epoch = max(int(num_train/self.batch_size),1)\n",
    "        \n",
    "        num_iterations = iterations_per_epoch *self.num_epochs\n",
    "        \n",
    "        for i in range(num_iterations):\n",
    "            self._step()\n",
    "            if self.verbose and i % self.print_every == 0:\n",
    "                print (\"(Iteration %d / %d) loss: %f\" % (i + 1, num_iterations, self.loss_history[-1]))\n",
    "            \n",
    "            epoch_end = ((i+1) % iterations_per_epoch) == 0\n",
    "            if epoch_end:\n",
    "                self.epoch +=1\n",
    "                for k in self.optim_configs:\n",
    "                    self.optim_configs[k]['learning_rate']*=self.lr_decay\n",
    "                \n",
    "            first_it = (i==0)\n",
    "            last_it = (i==num_iterations+1)\n",
    "            if first_it or last_it or epoch_end:\n",
    "                train_acc = self.check_accuracy(self.X_train,self.y_train,num_samples=1000)\n",
    "                val_acc = self.check_accuracy(self.X_val,self.y_val,num_samples=1000)\n",
    "                self.train_acc_history.append(train_acc)\n",
    "                self.val_acc_history.append(val_acc)\n",
    "                if self.verbose:\n",
    "                    print ('(Epoch %d / %d) train acc: %f; val_acc: %f' % (\n",
    "                     self.epoch, self.num_epochs, train_acc, val_acc))\n",
    "                \n",
    "                    if val_acc > self.best_val_acc:\n",
    "                        self.best_val_acc = val_acc\n",
    "                        self.best_params = {}\n",
    "                        for k,v in self.model.params.items():\n",
    "                            self.best_params[k] = v.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook solver.ipynb to script\n",
      "[NbConvertApp] Writing 5282 bytes to solver.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script solver.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
