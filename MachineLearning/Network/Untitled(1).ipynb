{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Load_CIFAR10\n",
    "import numpy as  np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Load_CIFAR10' from 'E:\\\\JupyterEnviroment\\\\Learning\\\\MachineLearning\\\\Network\\\\Load_CIFAR10.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imp import reload \n",
    "reload(Load_CIFAR10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "trainX,trainY,testX,testY = Load_CIFAR10.load_Data_Lables()\n",
    "#拉成一维向量\n",
    "trainX = trainX.reshape(trainX.shape[0],-1)\n",
    "testX = testX.reshape(testX.shape[0],-1)\n",
    "#标准化\n",
    "scaler = preprocessing.StandardScaler().fit(trainX)\n",
    "trainX = scaler.transform(trainX)\n",
    "testX = scaler.transform(testX)\n",
    "#给每个数据都要多加一维，作为偏置位\n",
    "train_bias = np.ones([trainX.shape[0],1],float)\n",
    "test_bias = np.ones([testX.shape[0],1],float)\n",
    "trainX = np.c_[trainX,train_bias]\n",
    "testX = np.c_[testX,test_bias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self,layers,alpha):\n",
    "        '''初始函数\n",
    "        layers:tuple类型,每一个元素表示该层的单元个数\n",
    "        alpha:学习率\n",
    "        '''\n",
    "        self.layers = layers\n",
    "        self.W = []\n",
    "        self.dropOut = [None for i in range(len(layers)-1)]\n",
    "         \n",
    "        self.alpha = alpha\n",
    "        self.out = [None for i in range(len(layers)-1)]\n",
    "        self.result = [None for i in range(len(layers))]#每一层运算后的结果\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            #输入层和输出层不需要权重\n",
    "            #要考虑偏置值\n",
    "            w = np.random.randn(layers[i]+1,layers[i+1]+1)\n",
    "            self.W.append(w)\n",
    "        \n",
    "        #最后一层输出层不需要加偏置项\n",
    "        w = np.random.randn(layers[-2]+1,layers[-1])\n",
    "        self.W.append(w)\n",
    "        \n",
    "        #转换一下类型\n",
    "        self.W = np.array(self.W)\n",
    "        self.result = np.array(self.result)\n",
    "\n",
    "        \n",
    "    def Relu(self,value):\n",
    "        '''激活函数采用整流线性单元，但是为了避免出现梯度饱和，采用渗漏型整流线性单元\n",
    "        value:上一层的输出入与隐藏层的权重乘积的结果\n",
    "        '''\n",
    "        v = []\n",
    "        for i in value:\n",
    "            d = [j if j>=0 else j*0.01 for j in i]\n",
    "            v.append(d)\n",
    "        return np.array(v)\n",
    "\n",
    "    def softMax(self,value):\n",
    "        max_dict = [ [np.max(i)] for i in value]\n",
    "        x = np.exp((value-max_dict))\n",
    "        \n",
    "        return np.array([ i/np.sum(i) for i in x])\n",
    "    \n",
    "    def fit(self,trainX,trainY,batch_size,reg,num):\n",
    "        num_train = trainX.shape[0]\n",
    "        for i in range(num):\n",
    "            #进行小批量训练，随机抽取一批数量为batch_size的数据集进行训练\n",
    "            batch_index = np.random.choice(num_train,batch_size)\n",
    "            batch_X = trainX[batch_index,:]\n",
    "            batch_Y = trainY[batch_index]\n",
    "            self.train(batch_X,batch_Y,0.2)\n",
    "            if i%100==0 :\n",
    "#                 print('W',self.W[-1])\n",
    "                print('loss',self.calculateLoss(trainX,trainY))\n",
    "    def getdropOutVector(self,size):\n",
    "        '''获取一次用于dropOut的向量Load_CIFAR10.py\n",
    "        size:表示几行几列\n",
    "        '''\n",
    "        _sum = 0\n",
    "        while _sum<=0:\n",
    "            dropOutV = np.random.rand(size[1])\n",
    "            dropOutV = dropOutV>0.5\n",
    "            _sum = sum(dropOutV)\n",
    "            \n",
    "        return dropOutV\n",
    "    \n",
    "    def predict(self,X):\n",
    "        out = X\n",
    "        #计算隐藏层\n",
    "        for i in range(1,len(self.layers)-1):\n",
    "            #上一层的输出与当前层的权重相乘\n",
    "            net = out.dot(self.W[i-1])*0.5\n",
    "            \n",
    "            #经过激活函数\n",
    "            out = self.Relu(net)\n",
    "        \n",
    "        #计算输出层\n",
    "        net = out.dot(self.W[-1])\n",
    "        out = self.softMax(net)\n",
    "        return out\n",
    "    \n",
    "    def calculateLoss(self,trainX,trainY):\n",
    "        predictions = self.predict(trainX)\n",
    "        \n",
    "        loss = sum(np.sum(trainY*np.log(predictions),1))\n",
    "        \n",
    "        return loss\n",
    "    def train(self,batch_X,batch_Y,reg):\n",
    "        self.result[0] = batch_X\n",
    "        \n",
    "        #计算隐藏层\n",
    "        for i in range(1,len(self.layers)-1):\n",
    "            \n",
    "            #上一层的输出与当前层的权重相乘\n",
    "            net = self.result[i-1].dot(self.W[i-1])\n",
    "            scaler = preprocessing.StandardScaler().fit(net)\n",
    "            net = scaler.transform(net)\n",
    "            \n",
    "            #生成当前层dropout向量\n",
    "            #保存到self.dropOut中去，用于反向传播\n",
    "            #再把Dropout向量乘上当前的输出\n",
    "            dropOutVec = self.getdropOutVector([1,net.shape[1]])\n",
    "            self.dropOut[i-1] = dropOutVec\n",
    "            net = net*dropOutVec\n",
    "            \n",
    "            #经过激活函数\n",
    "            out = self.Relu(net)\n",
    "            self.out[i-1] = out\n",
    "            self.result[i] = out\n",
    "\n",
    "        #计算输出层\n",
    "        net = self.result[-2].dot(self.W[-1])#因为数据没有归一化或者标准化化？导致数据过大，爆了。。\n",
    "        out = self.softMax(net)\n",
    "        self.result[-1] = out\n",
    "        #反向传播\n",
    "        #先是输出层的反向传播梯度更新\n",
    "        df_dout = out-batch_Y\n",
    "        \n",
    "        dout_dnet = []\n",
    "        for i in range(batch_X.shape[0]):\n",
    "            _dout_dnet = [(np.sum(self.result[-1][i])-self.result[-1][i][j])*self.result[-1][i][j]/(np.sum(self.result[-1][i])**2) for j in range(self.layers[-1])]\n",
    "            dout_dnet.append(_dout_dnet)\n",
    "        dout_dnet = np.array(dout_dnet)\n",
    "\n",
    "        D = [None for i in range(len(self.W))]#保存 df/dout *dout/dnet\n",
    "        D[-1] = df_dout*dout_dnet\n",
    "\n",
    "        #接着是隐藏层的\n",
    "        for i in range(len(self.W)-2,-1,-1):\n",
    "            dout_dnet = []\n",
    "            for j in self.out[i]:\n",
    "                _dout_dnet = [1 if m>=0 else 0.01 for m in j]\n",
    "                dout_dnet.append(_dout_dnet)\n",
    "                \n",
    "            dout_dnet = np.array(dout_dnet)\n",
    "    \n",
    "            D[i] = D[i+1].dot(self.W[i+1].T)*dout_dnet#注意点乘还是*乘\n",
    "        \n",
    "        \n",
    "        #为输入出增加一层全都是1的dropout层，输入层是不需要dropout的，为了公式一致所以加上一层都是为1的dropout层\n",
    "        last_dropoutVector = np.array([1.0 for i in range(self.W[-1].shape[-1])])\n",
    "        self.dropOut[-1] = last_dropoutVector\n",
    "          \n",
    "        #然后是梯度更新\n",
    "        for i in range(len(self.W)):\n",
    "            self.W[i] = self.W[i] - self.alpha*(self.W[i]*reg+1/batch_X.shape[0]*self.result[i].T.dot(D[i]*self.dropOut[i]))#注意点乘还是*乘\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits #手写数字数据集\n",
    "from sklearn.preprocessing import LabelBinarizer #标签二值化处理\n",
    "from sklearn.model_selection import train_test_split #训练和测试集分隔\n",
    "# 载入数据\n",
    "digits = load_digits()\n",
    "\n",
    "# 输入的数据\n",
    "X = digits.data\n",
    "X = X.reshape([X.shape[0],-1])\n",
    "# 标签数据\n",
    "T = digits.target\n",
    "# 数据切分，默认测试集占0.25\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,T)\n",
    "# (1797,) [0 1]\n",
    "labels_train = LabelBinarizer().fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 141762.907799532\n"
     ]
    }
   ],
   "source": [
    "bp_net = NeuralNetwork([X_train.shape[1]-1,16,10],0.1)\n",
    "bp_net.fit(trainX=X_train,trainY=labels_train,batch_size=256,num=100,reg=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bp_net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-53366417a856>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbp_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#这里有一个问题，如果训练的次数少了，会导致有很少部分的数字会出现一次都不会被检测到\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bp_net' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "output = bp_net.predict(X_test)\n",
    "predictions = np.argmax(output,axis=1)\n",
    "print(classification_report(predictions,y_test))\n",
    "#这里有一个问题，如果训练的次数少了，会导致有很少部分的数字会出现一次都不会被检测到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0227788ad90c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#失败。\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbp_net\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mbp_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mbp_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainX' is not defined"
     ]
    }
   ],
   "source": [
    "#失败。\n",
    "bp_net = NeuralNetwork([trainX.shape[1]-1,128,64,32,16,10],1e-3)\n",
    "bp_net.fit(trainX=trainX[:100],trainY=trainY[:100],batch_size=64,num=5000,reg=0.9)\n",
    "bp_net.predict(trainX[:100],trainY[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 8, 8)\n",
      "(1797, 64)\n",
      "(1797,)\n"
     ]
    }
   ],
   "source": [
    "#原版数据载入\n",
    "# # 载入数据\n",
    "# digits = load_digits()\n",
    "# print(digits.images.shape) #结果：(1797, 8, 8)\n",
    "\n",
    "# # 输入的数据\n",
    "# X = digits.data\n",
    "# X = X.reshape([X.shape[0],-1])\n",
    "# # 标签数据\n",
    "# T = digits.target\n",
    "# print(X.shape)\n",
    "# print(T.shape)\n",
    "# # print(X.shape, X[:2], '\\n')\n",
    "# # print(T.shape, T[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 8, 8)\n",
      "(1797, 64) [[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      "  15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "   0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "   0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n",
      " [ 0.  0.  0. 12. 13.  5.  0.  0.  0.  0.  0. 11. 16.  9.  0.  0.  0.  0.\n",
      "   3. 15. 16.  6.  0.  0.  0.  7. 15. 16. 16.  2.  0.  0.  0.  0.  1. 16.\n",
      "  16.  3.  0.  0.  0.  0.  1. 16. 16.  6.  0.  0.  0.  0.  1. 16. 16.  6.\n",
      "   0.  0.  0.  0.  0. 11. 16. 10.  0.  0.]] \n",
      "\n",
      "(1797,) [0 1]\n",
      "[4 1]\n",
      "[[0 0 0 0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]]\n",
      "iter: 1 acc: 0.10888888888888888\n",
      "iter: 1001 acc: 0.5177777777777778\n",
      "iter: 2001 acc: 0.6088888888888889\n",
      "iter: 3001 acc: 0.7888888888888889\n",
      "iter: 4001 acc: 0.8533333333333334\n",
      "iter: 5001 acc: 0.8933333333333333\n",
      "iter: 6001 acc: 0.9066666666666666\n",
      "iter: 7001 acc: 0.9377777777777778\n",
      "iter: 8001 acc: 0.9377777777777778\n",
      "iter: 9001 acc: 0.9444444444444444\n",
      "iter: 10001 acc: 0.9333333333333333\n",
      "[8 0 7 9 4 4 9 6 7 7 7 1 7 3 7 4 7 4 2 9 7 4 8 0 8 5 5 7 3 7 0 0 9 3 0 8 5\n",
      " 0 3 6 2 4 3 4 8 3 5 6 3 4 1 7 2 1 1 8 9 5 8 2 6 4 2 2 1 2 7 6 8 9 0 9 1 3\n",
      " 2 9 9 3 2 7 0 9 2 5 8 2 1 8 9 1 0 6 9 9 3 5 9 1 9 4 5 7 8 0 6 7 1 0 9 0 1\n",
      " 6 0 8 4 5 4 2 0 1 6 8 6 3 7 1 3 6 6 8 6 9 6 4 3 7 1 4 6 0 2 0 1 0 6 5 9 0\n",
      " 1 7 6 6 2 0 9 9 8 8 5 2 9 4 2 0 6 4 2 9 0 4 7 3 5 2 9 9 7 8 1 3 9 4 4 5 4\n",
      " 1 3 6 3 4 5 8 6 3 4 1 8 7 6 7 4 7 3 9 1 6 9 2 1 4 5 6 3 3 1 1 7 3 5 9 0 9\n",
      " 4 8 2 9 5 0 6 6 7 2 1 0 1 3 9 5 1 0 3 9 7 7 3 1 2 8 5 5 9 4 2 6 9 1 8 4 3\n",
      " 5 9 9 3 9 3 6 8 5 9 9 0 1 5 2 6 7 1 8 4 8 7 7 8 7 9 3 2 3 5 6 4 6 2 6 0 2\n",
      " 9 7 9 2 0 5 4 0 2 3 6 3 5 3 3 6 8 6 8 6 6 5 0 0 2 0 3 6 5 9 5 6 8 5 5 0 5\n",
      " 5 8 4 3 9 4 0 3 1 3 2 9 6 0 9 3 5 8 2 8 9 5 6 7 6 1 5 5 3 5 7 5 0 0 4 6 4\n",
      " 0 9 7 3 5 2 9 1 0 6 6 3 4 4 1 1 6 6 4 8 6 8 3 8 3 2 6 6 9 0 5 8 3 5 9 6 7\n",
      " 9 8 2 1 3 0 5 2 7 8 0 3 6 3 6 9 4 8 3 2 9 9 8 6 9 0 3 2 3 9 3 5 9 7 3 6 8\n",
      " 4 5 9 9 1 8]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.98      0.99        43\n",
      "          1       0.91      0.86      0.89        37\n",
      "          2       0.97      0.95      0.96        38\n",
      "          3       0.96      0.92      0.94        53\n",
      "          4       0.95      1.00      0.97        39\n",
      "          5       0.85      0.98      0.91        45\n",
      "          6       0.98      0.98      0.98        55\n",
      "          7       0.93      1.00      0.96        38\n",
      "          8       0.79      0.90      0.84        42\n",
      "          9       1.00      0.80      0.89        60\n",
      "\n",
      "avg / total       0.94      0.93      0.93       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits #手写数字数据集\n",
    "from sklearn.preprocessing import LabelBinarizer #标签二值化处理\n",
    "from sklearn.model_selection import train_test_split #训练和测试集分隔\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 载入数据\n",
    "digits = load_digits()\n",
    "print(digits.images.shape) #结果：(1797, 8, 8)\n",
    "\n",
    "# 输入的数据\n",
    "X = digits.data\n",
    "# 标签数据\n",
    "T = digits.target\n",
    "print(X.shape, X[:2], '\\n')\n",
    "print(T.shape, T[:2])\n",
    "\n",
    "# (1797,) [0 1]\n",
    "\n",
    "# 定义一个神经网络 64-100-10，隐藏层神经元单元为100,输出层神经元单元为10\n",
    "# 输入层至隐藏层的权值矩阵\n",
    "V = np.random.random([64,100])*2 -1\n",
    "# 隐藏层至输出层的权值矩阵\n",
    "W = np.random.random([100,10])*2 -1\n",
    "\n",
    "# 数据切分，默认测试集占0.25\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,T)\n",
    "\n",
    "# 标签二值化，独热编码\n",
    "# 1 -> 0100000000\n",
    "labels_train = LabelBinarizer().fit_transform(y_train)\n",
    "print(y_train[:2])\n",
    "print(labels_train[:2])\n",
    "#打印结果：\n",
    "#[5 9]\n",
    "#[[0 0 0 0 0 1 0 0 0 0]\n",
    " #[0 0 0 0 0 0 0 0 0 1]]\n",
    " \n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# 激活函数的导数\n",
    "def dsigmoid(x):\n",
    "    return x*(1-x)\n",
    "\n",
    "#预测值计算\n",
    "def predict(x):\n",
    "    L1 = sigmoid(np.dot(x,V))\n",
    "    L2 = sigmoid(np.dot(L1,W))\n",
    "    return L2\n",
    "\n",
    "#模型训练\n",
    "def train(X, T, steps=10000, lr=0.11):\n",
    "    global V,W\n",
    "    for n in range(steps + 1):\n",
    "        #从样本中随机选取一个数据\n",
    "        i = np.random.randint(X.shape[0])\n",
    "        x = X[i]\n",
    "        x = np.atleast_2d(x) #转换为2D矩阵\n",
    "        #BP算法公式\n",
    "        L1 = sigmoid(np.dot(x,V))\n",
    "        L2 = sigmoid(np.dot(L1,W))\n",
    "        \n",
    "        L2_delta = (T[i] - L2)*dsigmoid(L2)\n",
    "        L1_delta = L2_delta.dot(W.T)*dsigmoid(L1)\n",
    "        \n",
    "        W += lr * L1.T.dot(L2_delta)\n",
    "        V += lr * x.T.dot(L1_delta)\n",
    "        \n",
    "        if n%1000 == 0:\n",
    "            output = predict(X_test)\n",
    "            predictions = np.argmax(output, axis=1)\n",
    "            acc = np.mean(np.equal(predictions, y_test))\n",
    "            print('iter: ' + str(n + 1) + \" acc: \" + str(acc))\n",
    "\n",
    "train(X_train,labels_train,10000)\n",
    "#输出结果：\n",
    "# iter: 1 acc: 0.8644444444444445\n",
    "# iter: 1001 acc: 0.8733333333333333\n",
    "# iter: 2001 acc: 0.8688888888888889\n",
    "# ...\n",
    "# iter: 5001 acc: 0.8711111111111111\n",
    "# ...\n",
    "# iter: 22001 acc: 0.8666666666666667\n",
    "# iter: 23001 acc: 0.8711111111111111\n",
    "#....\n",
    "# iter: 28001 acc: 0.8711111111111111\n",
    "# iter: 29001 acc: 0.8666666666666667\n",
    "# iter: 30001 acc: 0.8711111111111111\n",
    "\n",
    "# 结果评估\n",
    "output = predict(X_test)\n",
    "predictions = np.argmax(output,axis=1)\n",
    "print(predictions)\n",
    "print(classification_report(predictions,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450,)\n",
      "(450,)\n"
     ]
    }
   ],
   "source": [
    "print(predictions.shape)\n",
    "print(y_test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
