{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Load_CIFAR10\n",
    "import numpy as  np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Load_CIFAR10' from 'E:\\\\JupyterEnviroment\\\\Learning\\\\MachineLearning\\\\Network\\\\Load_CIFAR10.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imp import reload \n",
    "reload(Load_CIFAR10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "trainX,trainY,testX,testY = Load_CIFAR10.load_Data_Lables()\n",
    "#拉成一维向量\n",
    "trainX = trainX.reshape(trainX.shape[0],-1)\n",
    "testX = testX.reshape(testX.shape[0],-1)\n",
    "#标准化\n",
    "scaler = preprocessing.StandardScaler().fit(trainX)\n",
    "trainX = scaler.transform(trainX)\n",
    "testX = scaler.transform(testX)\n",
    "#给每个数据都要多加一维，作为偏置位\n",
    "train_bias = np.ones([trainX.shape[0],1],float)\n",
    "test_bias = np.ones([testX.shape[0],1],float)\n",
    "trainX = np.c_[trainX,train_bias]\n",
    "testX = np.c_[testX,test_bias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self,layers,alpha):\n",
    "        '''初始函数\n",
    "        layers:tuple类型,每一个元素表示该层的单元个数\n",
    "        alpha:学习率\n",
    "        '''\n",
    "        self.layers = layers\n",
    "        self.W = []\n",
    "        self.dropOut = [None for i in range(len(layers)-1)]\n",
    "         \n",
    "        self.alpha = alpha\n",
    "        self.out = [None for i in range(len(layers)-1)]\n",
    "        self.result = [None for i in range(len(layers))]#每一层运算后的结果\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            #输入层和输出层不需要权重\n",
    "            #要考虑偏置值\n",
    "            w = np.random.randn(layers[i]+1,layers[i+1]+1)\n",
    "            self.W.append(w)\n",
    "        \n",
    "        #最后一层输出层不需要加偏置项\n",
    "        w = np.random.randn(layers[-2]+1,layers[-1])\n",
    "        self.W.append(w)\n",
    "        \n",
    "        #转换一下类型\n",
    "        self.W = np.array(self.W)\n",
    "        self.result = np.array(self.result)\n",
    "\n",
    "        \n",
    "    def Relu(self,value):\n",
    "        '''激活函数采用整流线性单元，但是为了避免出现梯度饱和，采用渗漏型整流线性单元\n",
    "        value:上一层的输出入与隐藏层的权重乘积的结果\n",
    "        '''\n",
    "        v = []\n",
    "        for i in value:\n",
    "            d = [j if j>=0 else j*0.01 for j in i]\n",
    "            v.append(d)\n",
    "        return np.array(v)\n",
    "\n",
    "    def softMax(self,value):\n",
    "        max_dict = [ [np.max(i)] for i in value]\n",
    "        x = np.exp((value-max_dict))\n",
    "        \n",
    "        return np.array([ i/np.sum(i) for i in x])\n",
    "    \n",
    "    def fit(self,trainX,trainY,batch_size,reg,num):\n",
    "        num_train = trainX.shape[0]\n",
    "        for i in range(num):\n",
    "            #进行小批量训练，随机抽取一批数量为batch_size的数据集进行训练\n",
    "            batch_index = np.random.choice(num_train,batch_size)\n",
    "            batch_X = trainX[batch_index,:]\n",
    "            batch_Y = trainY[batch_index]\n",
    "            self.train(batch_X,batch_Y,0.2)\n",
    "            if i%100==0 :\n",
    "#                 print('W',self.W[-1])\n",
    "                print('loss',self.calculateLoss(trainX,trainY))\n",
    "    def getdropOutVector(self,size):\n",
    "        '''获取一次用于dropOut的向量Load_CIFAR10.py\n",
    "        size:表示几行几列\n",
    "        '''\n",
    "        _sum = 0\n",
    "        while _sum<=0:\n",
    "            dropOutV = np.random.rand(size[1])\n",
    "            dropOutV = dropOutV>0.5\n",
    "            _sum = sum(dropOutV)\n",
    "            \n",
    "        return dropOutV\n",
    "    \n",
    "    def predict(self,X):\n",
    "        out = X\n",
    "        #计算隐藏层\n",
    "        for i in range(1,len(self.layers)-1):\n",
    "            #上一层的输出与当前层的权重相乘\n",
    "            net = out.dot(self.W[i-1])*0.5\n",
    "            \n",
    "            #经过激活函数\n",
    "            out = self.Relu(net)\n",
    "        \n",
    "        #计算输出层\n",
    "        net = out.dot(self.W[-1])\n",
    "        out = self.softMax(net)\n",
    "        return out\n",
    "    \n",
    "    def calculateLoss(self,trainX,trainY):\n",
    "        predictions = self.predict(trainX)\n",
    "        \n",
    "        loss = sum(np.sum(trainY*np.log(predictions),1))\n",
    "        \n",
    "        return loss\n",
    "    def train(self,batch_X,batch_Y,reg):\n",
    "        self.result[0] = batch_X\n",
    "        \n",
    "        #计算隐藏层\n",
    "        for i in range(1,len(self.layers)-1):\n",
    "            \n",
    "            #上一层的输出与当前层的权重相乘\n",
    "            net = self.result[i-1].dot(self.W[i-1])\n",
    "            scaler = preprocessing.StandardScaler().fit(net)\n",
    "            net = scaler.transform(net)\n",
    "            \n",
    "            #生成当前层dropout向量\n",
    "            #保存到self.dropOut中去，用于反向传播\n",
    "            #再把Dropout向量乘上当前的输出\n",
    "            dropOutVec = self.getdropOutVector([1,net.shape[1]])\n",
    "            self.dropOut[i-1] = dropOutVec\n",
    "            net = net*dropOutVec\n",
    "            \n",
    "            #经过激活函数\n",
    "            out = self.Relu(net)\n",
    "            self.out[i-1] = out\n",
    "            self.result[i] = out\n",
    "\n",
    "        #计算输出层\n",
    "        net = self.result[-2].dot(self.W[-1])#因为数据没有归一化或者标准化化？导致数据过大，爆了。。\n",
    "        out = self.softMax(net)\n",
    "        self.result[-1] = out\n",
    "        #反向传播\n",
    "        #先是输出层的反向传播梯度更新\n",
    "        df_dout = out-batch_Y\n",
    "        \n",
    "        dout_dnet = []\n",
    "        for i in range(batch_X.shape[0]):\n",
    "            _dout_dnet = [(np.sum(self.result[-1][i])-self.result[-1][i][j])*self.result[-1][i][j]/(np.sum(self.result[-1][i])**2) for j in range(self.layers[-1])]\n",
    "            dout_dnet.append(_dout_dnet)\n",
    "        dout_dnet = np.array(dout_dnet)\n",
    "\n",
    "        D = [None for i in range(len(self.W))]#保存 df/dout *dout/dnet\n",
    "        D[-1] = df_dout*dout_dnet\n",
    "\n",
    "        #接着是隐藏层的\n",
    "        for i in range(len(self.W)-2,-1,-1):\n",
    "            dout_dnet = []\n",
    "            for j in self.out[i]:\n",
    "                _dout_dnet = [1 if m>=0 else 0.01 for m in j]\n",
    "                dout_dnet.append(_dout_dnet)\n",
    "                \n",
    "            dout_dnet = np.array(dout_dnet)\n",
    "    \n",
    "            D[i] = D[i+1].dot(self.W[i+1].T)*dout_dnet#注意点乘还是*乘\n",
    "        \n",
    "        \n",
    "        #为输入出增加一层全都是1的dropout层，输入层是不需要dropout的，为了公式一致所以加上一层都是为1的dropout层\n",
    "        last_dropoutVector = np.array([1.0 for i in range(self.W[-1].shape[-1])])\n",
    "        self.dropOut[-1] = last_dropoutVector\n",
    "          \n",
    "        #然后是梯度更新\n",
    "        for i in range(len(self.W)):\n",
    "            self.W[i] = self.W[i] - self.alpha*(self.W[i]*reg+1/batch_X.shape[0]*self.result[i].T.dot(D[i]*self.dropOut[i]))#注意点乘还是*乘\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits #手写数字数据集\n",
    "from sklearn.preprocessing import LabelBinarizer #标签二值化处理\n",
    "from sklearn.model_selection import train_test_split #训练和测试集分隔\n",
    "# 载入数据\n",
    "digits = load_digits()\n",
    "\n",
    "# 输入的数据\n",
    "X = digits.data\n",
    "X = X.reshape([X.shape[0],-1])\n",
    "# 标签数据\n",
    "T = digits.target\n",
    "# 数据切分，默认测试集占0.25\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,T)\n",
    "# (1797,) [0 1]\n",
    "labels_train = LabelBinarizer().fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 141762.907799532\n"
     ]
    }
   ],
   "source": [
    "bp_net = NeuralNetwork([X_train.shape[1]-1,16,10],0.1)\n",
    "bp_net.fit(trainX=X_train,trainY=labels_train,batch_size=256,num=100,reg=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         1\n",
      "          1       0.00      0.00      0.00         0\n",
      "          2       1.00      0.19      0.32       196\n",
      "          3       0.60      0.37      0.46        83\n",
      "          4       0.88      0.65      0.75        46\n",
      "          5       0.00      0.00      0.00         0\n",
      "          6       0.00      0.00      0.00         0\n",
      "          7       0.00      0.00      0.00         0\n",
      "          8       0.00      0.00      0.00         0\n",
      "          9       0.50      0.19      0.27       124\n",
      "\n",
      "avg / total       0.77      0.27      0.37       450\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "output = bp_net.predict(X_test)\n",
    "predictions = np.argmax(output,axis=1)\n",
    "print(classification_report(predictions,y_test))\n",
    "#这里有一个问题，如果训练的次数少了，会导致有很少部分的数字会出现一次都不会被检测到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss -413\n",
      "acc 0.17\n",
      "loss -198\n",
      "acc 0.17\n",
      "loss -157\n",
      "acc 0.19\n",
      "loss -142\n",
      "acc 0.18\n",
      "loss -135\n",
      "acc 0.19\n",
      "loss -127\n",
      "acc 0.21\n",
      "loss -121\n",
      "acc 0.22\n",
      "loss -116\n",
      "acc 0.24\n",
      "loss -115\n",
      "acc 0.23\n",
      "loss -114\n",
      "acc 0.24\n",
      "loss -112\n",
      "acc 0.26\n",
      "loss -112\n",
      "acc 0.28\n",
      "loss -112\n",
      "acc 0.28\n",
      "loss -112\n",
      "acc 0.27\n",
      "loss -112\n",
      "acc 0.29\n",
      "loss -113\n",
      "acc 0.28\n",
      "loss -112\n",
      "acc 0.3\n",
      "loss -113\n",
      "acc 0.29\n",
      "loss -113\n",
      "acc 0.3\n",
      "loss -114\n",
      "acc 0.3\n",
      "loss -115\n",
      "acc 0.3\n",
      "loss -116\n",
      "acc 0.31\n",
      "loss -117\n",
      "acc 0.32\n",
      "loss -118\n",
      "acc 0.32\n",
      "loss -119\n",
      "acc 0.32\n",
      "loss -120\n",
      "acc 0.31\n",
      "loss -120\n",
      "acc 0.31\n",
      "loss -121\n",
      "acc 0.3\n",
      "loss -122\n",
      "acc 0.3\n",
      "loss -123\n",
      "acc 0.3\n",
      "loss -124\n",
      "acc 0.29\n",
      "loss -125\n",
      "acc 0.29\n",
      "loss -126\n",
      "acc 0.29\n",
      "loss -127\n",
      "acc 0.29\n",
      "loss -128\n",
      "acc 0.29\n",
      "loss -129\n",
      "acc 0.29\n",
      "loss -130\n",
      "acc 0.29\n",
      "loss -131\n",
      "acc 0.29\n",
      "loss -132\n",
      "acc 0.29\n",
      "loss -133\n",
      "acc 0.29\n",
      "loss -134\n",
      "acc 0.29\n",
      "loss -135\n",
      "acc 0.28\n",
      "loss -136\n",
      "acc 0.28\n",
      "loss -137\n",
      "acc 0.28\n",
      "loss -138\n",
      "acc 0.28\n",
      "loss -139\n",
      "acc 0.28\n",
      "loss -140\n",
      "acc 0.28\n",
      "loss -141\n",
      "acc 0.28\n",
      "loss -142\n",
      "acc 0.28\n",
      "loss -142\n",
      "acc 0.28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.28"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#失败。\n",
    "bp_net = NeuralNetwork([trainX.shape[1]-1,128,64,32,16,10],1e-3)\n",
    "bp_net.fit(trainX=trainX[:100],trainY=trainY[:100],batch_size=64,num=5000,reg=0.9)\n",
    "bp_net.predict(trainX[:100],trainY[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 8, 8)\n",
      "(1797, 64)\n",
      "(1797,)\n"
     ]
    }
   ],
   "source": [
    "#原版数据载入\n",
    "# # 载入数据\n",
    "# digits = load_digits()\n",
    "# print(digits.images.shape) #结果：(1797, 8, 8)\n",
    "\n",
    "# # 输入的数据\n",
    "# X = digits.data\n",
    "# X = X.reshape([X.shape[0],-1])\n",
    "# # 标签数据\n",
    "# T = digits.target\n",
    "# print(X.shape)\n",
    "# print(T.shape)\n",
    "# # print(X.shape, X[:2], '\\n')\n",
    "# # print(T.shape, T[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 8, 8)\n",
      "(1797, 64) [[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      "  15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "   0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "   0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n",
      " [ 0.  0.  0. 12. 13.  5.  0.  0.  0.  0.  0. 11. 16.  9.  0.  0.  0.  0.\n",
      "   3. 15. 16.  6.  0.  0.  0.  7. 15. 16. 16.  2.  0.  0.  0.  0.  1. 16.\n",
      "  16.  3.  0.  0.  0.  0.  1. 16. 16.  6.  0.  0.  0.  0.  1. 16. 16.  6.\n",
      "   0.  0.  0.  0.  0. 11. 16. 10.  0.  0.]] \n",
      "\n",
      "(1797,) [0 1]\n",
      "[0 8]\n",
      "[[1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0]]\n",
      "iter: 1 acc: 0.14444444444444443\n",
      "iter: 1001 acc: 0.5422222222222223\n",
      "iter: 2001 acc: 0.5733333333333334\n",
      "iter: 3001 acc: 0.5711111111111111\n",
      "iter: 4001 acc: 0.5822222222222222\n",
      "iter: 5001 acc: 0.6044444444444445\n",
      "[3 3 5 7 3 7 9 9 7 9 7 9 9 6 7 6 6 9 5 7 3 0 6 0 6 6 7 3 6 9 3 0 3 6 5 9 7\n",
      " 5 4 6 0 3 6 6 4 3 3 3 6 4 7 5 3 9 4 6 7 3 6 3 7 9 3 3 5 6 4 4 9 0 4 4 4 5\n",
      " 6 9 3 9 9 5 3 9 0 6 6 9 6 3 0 6 7 9 4 4 6 0 7 6 3 6 4 4 0 4 9 0 0 5 6 9 6\n",
      " 9 0 7 3 7 3 4 0 4 9 6 7 3 7 3 4 3 6 7 7 0 4 3 3 9 3 3 0 9 9 0 6 5 0 4 4 4\n",
      " 7 9 9 5 4 0 6 9 0 7 3 3 9 4 9 0 6 6 3 0 6 9 3 0 3 7 3 0 3 0 9 5 6 4 3 4 7\n",
      " 9 9 5 3 7 5 4 4 0 7 4 4 4 6 7 9 3 0 0 9 4 3 7 9 4 6 7 4 0 7 4 4 4 7 5 0 6\n",
      " 7 7 9 3 0 7 7 3 4 6 4 6 9 9 3 9 7 0 3 6 3 3 6 4 4 7 7 3 6 6 9 7 9 6 4 4 9\n",
      " 6 4 7 3 3 7 0 3 0 6 3 7 3 7 4 7 9 3 0 3 3 3 9 3 3 4 4 4 4 6 3 7 4 9 9 3 5\n",
      " 3 5 0 4 9 0 6 6 7 7 4 3 0 3 3 0 6 4 6 4 4 3 9 3 4 3 6 4 7 7 7 3 5 9 6 5 3\n",
      " 7 3 9 3 4 9 3 7 7 5 4 3 3 3 6 3 5 4 5 3 4 9 6 4 6 4 7 3 9 4 6 5 4 3 5 9 9\n",
      " 0 4 9 3 3 6 9 7 6 6 5 5 3 0 9 4 3 3 4 3 4 3 5 0 3 5 7 4 6 3 3 5 9 7 6 7 3\n",
      " 0 9 4 3 6 3 7 6 0 3 3 3 3 0 5 9 4 4 7 3 9 0 4 4 4 0 9 6 3 3 7 6 3 3 4 7 3\n",
      " 6 0 9 3 6 4]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.94      0.97        47\n",
      "          1       0.00      0.00      0.00         0\n",
      "          2       0.00      0.00      0.00         0\n",
      "          3       1.00      0.51      0.68       103\n",
      "          4       1.00      0.64      0.78        77\n",
      "          5       0.20      0.30      0.24        30\n",
      "          6       1.00      0.63      0.77        67\n",
      "          7       0.94      0.75      0.84        61\n",
      "          8       0.00      0.00      0.00         0\n",
      "          9       0.85      0.45      0.59        65\n",
      "\n",
      "avg / total       0.92      0.60      0.72       450\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits #手写数字数据集\n",
    "from sklearn.preprocessing import LabelBinarizer #标签二值化处理\n",
    "from sklearn.model_selection import train_test_split #训练和测试集分隔\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 载入数据\n",
    "digits = load_digits()\n",
    "print(digits.images.shape) #结果：(1797, 8, 8)\n",
    "\n",
    "# 输入的数据\n",
    "X = digits.data\n",
    "# 标签数据\n",
    "T = digits.target\n",
    "print(X.shape, X[:2], '\\n')\n",
    "print(T.shape, T[:2])\n",
    "\n",
    "# (1797,) [0 1]\n",
    "\n",
    "# 定义一个神经网络 64-100-10，隐藏层神经元单元为100,输出层神经元单元为10\n",
    "# 输入层至隐藏层的权值矩阵\n",
    "V = np.random.random([64,100])*2 -1\n",
    "# 隐藏层至输出层的权值矩阵\n",
    "W = np.random.random([100,10])*2 -1\n",
    "\n",
    "# 数据切分，默认测试集占0.25\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,T)\n",
    "\n",
    "# 标签二值化，独热编码\n",
    "# 1 -> 0100000000\n",
    "labels_train = LabelBinarizer().fit_transform(y_train)\n",
    "print(y_train[:2])\n",
    "print(labels_train[:2])\n",
    "#打印结果：\n",
    "#[5 9]\n",
    "#[[0 0 0 0 0 1 0 0 0 0]\n",
    " #[0 0 0 0 0 0 0 0 0 1]]\n",
    " \n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# 激活函数的导数\n",
    "def dsigmoid(x):\n",
    "    return x*(1-x)\n",
    "\n",
    "#预测值计算\n",
    "def predict(x):\n",
    "    L1 = sigmoid(np.dot(x,V))\n",
    "    L2 = sigmoid(np.dot(L1,W))\n",
    "    return L2\n",
    "\n",
    "#模型训练\n",
    "def train(X, T, steps=10000, lr=0.11):\n",
    "    global V,W\n",
    "    for n in range(steps + 1):\n",
    "        #从样本中随机选取一个数据\n",
    "        i = np.random.randint(X.shape[0])\n",
    "        x = X[i]\n",
    "        x = np.atleast_2d(x) #转换为2D矩阵\n",
    "        #BP算法公式\n",
    "        L1 = sigmoid(np.dot(x,V))\n",
    "        L2 = sigmoid(np.dot(L1,W))\n",
    "        \n",
    "        L2_delta = (T[i] - L2)*dsigmoid(L2)\n",
    "        L1_delta = L2_delta.dot(W.T)*dsigmoid(L1)\n",
    "        \n",
    "        W += lr * L1.T.dot(L2_delta)\n",
    "        V += lr * x.T.dot(L1_delta)\n",
    "        \n",
    "        if n%1000 == 0:\n",
    "            output = predict(X_test)\n",
    "            predictions = np.argmax(output, axis=1)\n",
    "            acc = np.mean(np.equal(predictions, y_test))\n",
    "            print('iter: ' + str(n + 1) + \" acc: \" + str(acc))\n",
    "\n",
    "train(X_train,labels_train,5000)\n",
    "#输出结果：\n",
    "# iter: 1 acc: 0.8644444444444445\n",
    "# iter: 1001 acc: 0.8733333333333333\n",
    "# iter: 2001 acc: 0.8688888888888889\n",
    "# ...\n",
    "# iter: 5001 acc: 0.8711111111111111\n",
    "# ...\n",
    "# iter: 22001 acc: 0.8666666666666667\n",
    "# iter: 23001 acc: 0.8711111111111111\n",
    "#....\n",
    "# iter: 28001 acc: 0.8711111111111111\n",
    "# iter: 29001 acc: 0.8666666666666667\n",
    "# iter: 30001 acc: 0.8711111111111111\n",
    "\n",
    "# 结果评估\n",
    "output = predict(X_test)\n",
    "predictions = np.argmax(output,axis=1)\n",
    "print(predictions)\n",
    "print(classification_report(predictions,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450,)\n",
      "(450,)\n"
     ]
    }
   ],
   "source": [
    "print(predictions.shape)\n",
    "print(y_test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
