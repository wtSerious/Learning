{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Load_CIFAR10\n",
    "import numpy as  np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Load_CIFAR10' from 'F:\\\\JupyterNoteBookWorkenEnviroment\\\\MachineLearning\\\\NetWork\\\\Load_CIFAR10.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imp import reload \n",
    "reload(Load_CIFAR10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "trainX,trainY,testX,testY = Load_CIFAR10.load_Data_Lables()\n",
    "#拉成一维向量\n",
    "trainX = trainX.reshape(trainX.shape[0],-1)\n",
    "testX = testX.reshape(testX.shape[0],-1)\n",
    "#标准化\n",
    "scaler = preprocessing.StandardScaler().fit(trainX)\n",
    "trainX = scaler.transform(trainX)\n",
    "testX = scaler.transform(testX)\n",
    "#给每个数据都要多加一维，作为偏置位\n",
    "train_bias = np.ones([trainX.shape[0],1],float)\n",
    "test_bias = np.ones([testX.shape[0],1],float)\n",
    "trainX = np.c_[trainX,train_bias]\n",
    "testX = np.c_[testX,test_bias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(np.max(trainY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self,layers,alpha):\n",
    "        '''初始函数\n",
    "        layers:tuple类型,每一个元素表示该层的单元个数\n",
    "        alpha:学习率\n",
    "        '''\n",
    "        self.layers = layers\n",
    "        self.W = []\n",
    "        self.dropOut = [None for i in range(len(layers)-1)]\n",
    "         \n",
    "        self.alpha = alpha\n",
    "        self.out = [None for i in range(len(layers)-1)]\n",
    "        self.result = [None for i in range(len(layers))]#每一层运算后的结果\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            #输入层和输出层不需要权重\n",
    "            #要考虑偏置值\n",
    "            w = np.random.randn(layers[i]+1,layers[i+1]+1)\n",
    "            self.W.append(w)\n",
    "        \n",
    "        #最后一层输出层不需要加偏置项\n",
    "        w = np.random.randn(layers[-2]+1,layers[-1])\n",
    "        self.W.append(w)\n",
    "        \n",
    "        #转换一下类型\n",
    "        self.W = np.array(self.W)\n",
    "        self.result = np.array(self.result)\n",
    "\n",
    "        \n",
    "    def Relu(self,value):\n",
    "        '''激活函数采用整流线性单元，但是为了避免出现梯度饱和，采用渗漏型整流线性单元\n",
    "        value:上一层的输出入与隐藏层的权重乘积的结果\n",
    "        '''\n",
    "        v = []\n",
    "        for i in value:\n",
    "            d = [j if j>=0 else j*0.01 for j in i]\n",
    "            v.append(d)\n",
    "        return np.array(v)\n",
    "\n",
    "    def softMax(self,value):\n",
    "        x = np.exp(value)\n",
    "        return np.array([ i/np.sum(i) for i in x])\n",
    "    \n",
    "    def fit(self,trainX,trainY,batch_size,reg,num):\n",
    "        num_train = trainX.shape[0]\n",
    "        for i in range(num):\n",
    "            #进行小批量训练，随机抽取一批数量为batch_size的数据集进行训练\n",
    "            batch_index = np.random.choice(num_train,batch_size)\n",
    "            batch_X = trainX[batch_index,:]\n",
    "            batch_Y = trainY[batch_index]\n",
    "            self.train(batch_X,batch_Y,0.2)\n",
    "            print('loss',self.calculateLoss(trainX,trainY))\n",
    "        \n",
    "    def getdropOutVector(self,size):\n",
    "        '''获取一次用于dropOut的向量Load_CIFAR10.py\n",
    "        size:表示几行几列\n",
    "        '''\n",
    "        return np.random.randint(0,2,size)\n",
    "    \n",
    "    def calculateLoss(self,trainX,trainY):\n",
    "        out = trainX\n",
    "        #计算隐藏层\n",
    "        for i in range(1,len(self.layers)-1):\n",
    "            #上一层的输出与当前层的权重相乘\n",
    "            net = out.dot(self.W[i-1])\n",
    "            \n",
    "            scaler = preprocessing.StandardScaler().fit(net)\n",
    "            net = scaler.transform(net)\n",
    "            #生成当前层dropout向量\n",
    "            #保存到self.dropOut中去，用于反向传播\n",
    "            #再把Dropout向量乘上当前的输出\n",
    "#             dropOutVec = self.getdropOutVector([1,net.shape[1]])\n",
    "#             net = net*dropOutVec\n",
    "            \n",
    "            #经过激活函数\n",
    "            out = self.Relu(net)\n",
    "        \n",
    "        #计算输出层\n",
    "        net = out.dot(self.W[-1])#因为数据没有归一化或者标准化化？导致数据过大，爆了。。\n",
    "        out = self.softMax(net)\n",
    "        \n",
    "        loss = sum([np.log(out[i][trainY[i]]) for i in range(out.shape[0])])\n",
    "        regular_term = self.W**2\n",
    "        regular_term = [np.sum(i) for i in regular_term]\n",
    "        regular_term = np.sum(regular_term)\n",
    "        \n",
    "        return int(loss)\n",
    "    \n",
    "    def train(self,batch_X,batch_Y,reg):\n",
    "        self.result[0] = batch_X\n",
    "        \n",
    "        #计算隐藏层\n",
    "        for i in range(1,len(self.layers)-1):\n",
    "            \n",
    "            #上一层的输出与当前层的权重相乘\n",
    "            net = self.result[i-1].dot(self.W[i-1])\n",
    "            \n",
    "            scaler = preprocessing.StandardScaler().fit(net)\n",
    "            net = scaler.transform(net)\n",
    "            \n",
    "            #生成当前层dropout向量\n",
    "            #保存到self.dropOut中去，用于反向传播\n",
    "            #再把Dropout向量乘上当前的输出\n",
    "            dropOutVec = self.getdropOutVector([1,net.shape[1]])\n",
    "            self.dropOut[i-1] = dropOutVec\n",
    "            net = net*dropOutVec\n",
    "            \n",
    "            #经过激活函数\n",
    "            out = self.Relu(net)\n",
    "            self.out[i-1] = out\n",
    "            self.result[i] = out\n",
    "        \n",
    "        #计算输出层\n",
    "        net = self.result[-2].dot(self.W[-1])#因为数据没有归一化或者标准化化？导致数据过大，爆了。。\n",
    "        out = self.softMax(net)\n",
    "        self.result[-1] = out\n",
    "        \n",
    "        #反向传播\n",
    "        #先是输出层的反向传播梯度更新\n",
    "        df_dout = np.array([ [self.result[-1][i][batch_Y[i]]] for i in range(self.result[-1].shape[0])])\n",
    "        df_dout = 1.0/df_dout\n",
    "        \n",
    "        dout_dnet = []\n",
    "        for i in range(batch_X.shape[0]):\n",
    "            _dout_dnet = [(np.sum(self.result[-1][i])-self.result[-1][i][j])*self.result[-1][i][j]/(np.sum(self.result[-1][i])**2) for j in range(self.layers[-1])]\n",
    "            dout_dnet.append(_dout_dnet)\n",
    "        dout_dnet = np.array(dout_dnet)\n",
    "\n",
    "        D = [None for i in range(len(self.W))]#保存 df/dout *dout/dnet\n",
    "        D[-1] = df_dout*dout_dnet\n",
    "        \n",
    "        #接着是隐藏层的\n",
    "        for i in range(len(self.W)-2,-1,-1):\n",
    "            dout_dnet = []\n",
    "            for j in self.out[i]:\n",
    "                _dout_dnet = [1 if m>=0 else 0.01 for m in j]\n",
    "                dout_dnet.append(_dout_dnet)\n",
    "                \n",
    "            dout_dnet = np.array(dout_dnet)\n",
    "    \n",
    "            D[i] = D[i+1].dot(self.W[i+1].T)*dout_dnet#注意点乘还是*乘\n",
    "        \n",
    "        #为输入出增加一层全都是1的dropout层，输入层是不需要dropout的，为了公式一致所以加上一层都是为1的dropout层\n",
    "        last_dropoutVector = np.array([1.0 for i in range(self.W[-1].shape[-1])])\n",
    "        self.dropOut[-1] = last_dropoutVector\n",
    "          \n",
    "        #然后是梯度更新\n",
    "        for i in range(len(self.W)):\n",
    "            print(self.result[i].shape)\n",
    "            print(D[i].shape)\n",
    "            self.W[i] = self.W[i] - self.alpha*(self.W[i]*reg+self.result[i].T.dot(D[i]*self.dropOut[i]))#注意点乘还是*乘\n",
    "            print('dw',(self.alpha*(self.W[i]*reg+self.result[i].T.dot(D[i]*self.dropOut[i]))).shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 3073)\n",
      "(10, 65)\n",
      "dw (3073, 65)\n",
      "(10, 65)\n",
      "(10, 33)\n",
      "dw (65, 33)\n",
      "(10, 33)\n",
      "(10, 17)\n",
      "dw (33, 17)\n",
      "(10, 17)\n",
      "(10, 10)\n",
      "dw (17, 10)\n",
      "loss -22181\n"
     ]
    }
   ],
   "source": [
    "bp_net = NeuralNetwork([trainX.shape[1]-1,64,32,16,10],2e-6)\n",
    "bp_net.fit(trainX=trainX[:5000],trainY=trainY[:5000],batch_size=10,num=1,reg=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [[1,2,3],[2,3,4]]\n",
    "# b = [j if j<2 else 0.01*j for i in a for j in i ]\n",
    "# print(b)\n",
    "a = np.array([[1,2,3,4],[1,2,3]])\n",
    "print(np.sum(a**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 1.]\n",
      " [1. 2. 1.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# c = [1,2,3]\n",
    "# c = np.exp(c)\n",
    "# print(c)\n",
    "# _sum = np.sum(c)\n",
    "# print(c/_sum)\n",
    "# c = [[1,2,3],[2,3,4]]\n",
    "# print(np.sum(c,1))\n",
    "# help(np.full)\n",
    "\n",
    "# d = [ (1.0*3/(2*2)) for i in range(2) for i in range(2)]\n",
    "# e = np.atleast_2d(d)\n",
    "# d = [ (1.0*3/(2*2)) for i in range(2) for i in range(2)]\n",
    "# e = np.insert(e,0,d,0)\n",
    "# print(e)\n",
    "\n",
    "# e = [[1,2],[2,3],[3,4]]\n",
    "# e = np.array(e)\n",
    "# d = [[0,0],[-1,-2],[-2,-3]]\n",
    "# d = np.array(d)\n",
    "\n",
    "# print(e*d)\n",
    "d = np.array([[1,2],[1,2]])\n",
    "c = np.ones([d.shape[0],1],float)\n",
    "e = np.c_[d,c]\n",
    "print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-d5e8e100ef1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "a = [1,2,3,4]\n",
    "b = [1,2,3,4]\n",
    "for i,j in a,b:\n",
    "    print(i,j)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
