{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet \n",
    "因为利用 torchvision.datasets 的数据集时，下载的太慢了，所以想要直接载入自己本地的 CIFAR10 数据集。于是就学习了一下。\n",
    "\n",
    "Pytorch 中加载数据首先是弄数据集 DataSet,然后利用 DataLoader 进行数据的载入。\n",
    "\n",
    "我们要继承 from torch.utils.data import Dataset 的Dataset类，然后主要是重写 __init__，__getitem__和_len方法。可以仿照官网给的 CIFAR10 数据集的源码来仿写。注意在 getiem 中实现对数据集的预处理，也就是传进来的 torchvision.transforms 还有 __getitem__() 方法同时返回 data和target,就是元组，数据集和数据的载入在 pytorch 中都是用了元组保存。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "在定义数据集之后，就是要用 DataLoader 进行数据的载入了。在 DataLoader 中主要注意 batch_size，sampler 和 collate_fn。\n",
    "batch_size 就会把原本的数据分成一个个小 batch_size 再返回。sampler 决定了如何取数据。在官网中的 DataLoader 里有这么一段\n",
    "\n",
    "```python\n",
    "if batch_sampler is None:\n",
    "            if sampler is None:\n",
    "                if shuffle:\n",
    "                    sampler = RandomSampler(dataset)\n",
    "                else:\n",
    "                    sampler = SequentialSampler(dataset)\n",
    "            batch_sampler = BatchSampler(sampler, batch_size, drop_last)\n",
    "        self.sampler = sampler\n",
    "        self.batch_sampler = batch_sampler\n",
    "        self.__initialized = True\n",
    "```\n",
    "在 DataLoader 中 shuffle 是指定在载入数据时，是否混乱载入，我们可以看到，混乱载入就是用了一个 RandomSampler 在加载我们的数据集。\n",
    "而不混乱载入就是利用 SequentialSampler 按顺序的载入数据。Sampler 里面是怎么实现的等下再看。注意这里有 sampler 和 batch_sampler，首先就是先用 sampler 取数据，在利用 batchSampler 按 batch_size 大小再分。 在这里注意 sampler 和 batch_sampler 返回的数据都是迭代器类型。\n",
    "\n",
    "然后我们看看这些 Sampler.\n",
    "``` \n",
    "    class SequentialSampler(Sampler):\n",
    "    def __init__(self, data_source):\n",
    "        self.data_source = data_source\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(range(len(self.data_source)))\n",
    "\n",
    "[docs]class RandomSampler(Sampler):\n",
    "    def __init__(self, data_source, replacement=False, num_samples=None):\n",
    "        self.data_source = data_source\n",
    "        self.replacement = replacement\n",
    "        self._num_samples = num_samples\n",
    "\n",
    "    def __iter__(self):\n",
    "        n = len(self.data_source)\n",
    "        if self.replacement:\n",
    "            return iter(torch.randint(high=n, size=(self.num_samples,), dtype=torch.int64).tolist())\n",
    "        return iter(torch.randperm(n).tolist())\n",
    "```\n",
    "删除了一下代码。我们可以发现 Sampler 就是返回一个 index 列表迭代器，然后就可以根据索引取数据，就达到了如何取数据的效果。\n",
    "\n",
    "再看一下 BatchSampler:\n",
    "```\n",
    "[docs]class BatchSampler(Sampler):\n",
    "    Example:\n",
    "        >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))\n",
    "        [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n",
    "        >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))\n",
    "        [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n",
    "    \"\"\"\n",
    "    def __init__(self, sampler, batch_size, drop_last):\n",
    "\n",
    "        self.sampler = sampler\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch = []\n",
    "        for idx in self.sampler:\n",
    "            batch.append(idx)\n",
    "            if len(batch) == self.batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        if len(batch) > 0 and not self.drop_last:\n",
    "            yield batch\n",
    "```\n",
    "官网还再上面给了一个例子。我们可以看到在 __iter__ 中，batchSamper 是用上一个 sampler 返回的 index 迭代器，然后根据 batch_size 把索引再组合一遍，返回的就是一个个块，块里就是索引值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## collate_fn\n",
    "就是在取数据的时候如果我要数据进行一些其他的操作，可以自定义处理过程。我们看官网给的一个例子：\n",
    "``` \n",
    "class SimpleCustomBatch:\n",
    "    def __init__(self, data):\n",
    "        transposed_data = list(zip(*data))\n",
    "        self.inp = torch.stack(transposed_data[0], 0)\n",
    "        self.tgt = torch.stack(transposed_data[1], 0)\n",
    "\n",
    "    def pin_memory(self):\n",
    "        self.inp = self.inp.pin_memory()\n",
    "        self.tgt = self.tgt.pin_memory()\n",
    "        return self\n",
    "\n",
    "def collate_wrapper(batch):\n",
    "    return SimpleCustomBatch(batch)\n",
    "inps = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\n",
    "tgts = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\n",
    "dataset = TensorDataset(inps, tgts)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=2, collate_fn=collate_wrapper,\n",
    "                    pin_memory=True)\n",
    "\n",
    "for batch_ndx, sample in enumerate(loader):\n",
    "    print(sample.inp.is_pinned())\n",
    "    print(sample.tgt.is_pinned())\n",
    "```\n",
    "在这里想提一点，在 SimpleCustomBatch 中拿到的数据是 [($x_{1},y_{1}$),($x_{2},y_{2}$)] 这种样子的，而 DataLoader 返回的数据是\n",
    "([$x_{1},x_{2}...$],[$y_{1},y_{2}..$]) 这样子的，如果再有 batch_size 的话，就是([batch_index],[$x_{1},x_{2}...$],[$y_{1},y_{2}..$])。所以在 SimpleCustomBatch 中有一个解包，并且返回的都是解包后的元组。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
